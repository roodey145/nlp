{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1fbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3dc776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "\n",
    "# Core libraries\n",
    "!pip install torch --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install tqdm --quiet\n",
    "\n",
    "# Augmentation dependency\n",
    "!pip install nltk --quiet\n",
    "\n",
    "# Download WordNet resources\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306347c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers nltk datasets torch tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dbb31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Epoch 1: 100%|██████████| 534/534 [01:02<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.4904632152588556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 534/534 [01:03<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.5095367847411444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 534/534 [01:02<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.5340599455040872\n",
      "TEST accuracy: 0.5466063348416289\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# BERT + BiLSTM + Synonym Augmentation\n",
    "# Correct, full, working version\n",
    "# ======================================\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Settings\n",
    "# ----------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 64\n",
    "BATCH = 16\n",
    "EPOCHS = 3\n",
    "AUG = True\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Synonym augmentation\n",
    "# ----------------------------------------\n",
    "def synonym_augment(text, p=0.1):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "\n",
    "    for w in words:\n",
    "        if random.random() < p:\n",
    "            syns = wn.synsets(w)\n",
    "            if syns:\n",
    "                lemmas = syns[0].lemma_names()\n",
    "                cand = [l.replace(\"_\", \" \") for l in lemmas if l.lower() != w.lower()]\n",
    "                if cand:\n",
    "                    new_words.append(random.choice(cand))\n",
    "                    continue\n",
    "        new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load SST-5 from HuggingFace\n",
    "# ----------------------------------------\n",
    "dataset = load_dataset(\"SetFit/sst5\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])[[\"text\", \"label\"]]\n",
    "dev_df   = pd.DataFrame(dataset[\"validation\"])[[\"text\", \"label\"]]\n",
    "test_df  = pd.DataFrame(dataset[\"test\"])[[\"text\", \"label\"]]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Dataset class\n",
    "# ----------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class SST5Dataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx].text\n",
    "        label = self.df.iloc[idx].label\n",
    "\n",
    "        # Best practice: ONLY augment some samples, not all\n",
    "        if self.augment and random.random() < 0.3:\n",
    "            text = synonym_augment(text, p=0.1)\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(SST5Dataset(train_df, augment=True), batch_size=BATCH, shuffle=True)\n",
    "dev_loader   = DataLoader(SST5Dataset(dev_df, augment=False), batch_size=32)\n",
    "test_loader  = DataLoader(SST5Dataset(test_df, augment=False), batch_size=32)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Model: BERT → BiLSTM → FC\n",
    "# ----------------------------------------\n",
    "class BERT_BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden=256, layers=1, classes=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden * 2, classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.bert(ids, attention_mask=mask).last_hidden_state\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.fc(final)\n",
    "\n",
    "\n",
    "model = BERT_BiLSTM().to(DEVICE)\n",
    "\n",
    "# UNFREEZE BERT explicitly\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Optimizer (very important!)\n",
    "# Use SEPARATE learning rates\n",
    "# ----------------------------------------\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.bert.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.lstm.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": model.fc.parameters(),   \"lr\": 1e-4},\n",
    "])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Evaluation\n",
    "# ----------------------------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            mask = batch[\"mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            preds = logits.argmax(1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Training loop\n",
    "# ----------------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    model.bert.train()\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        mask = batch[\"mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Dev accuracy:\", evaluate(dev_loader))\n",
    "\n",
    "print(\"TEST accuracy:\", evaluate(test_loader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
