{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae6899e-8231-41ec-9fe1-5eb5e22a97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a58c4429-d043-4f6f-8764-20a079e047c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"SetFit/sst5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefd57eb-b4b9-4d20-82b8-04034073fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['very positive', 'negative', 'negative', 'neutral', 'positive'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"label_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1cb86fc-053a-48e8-a2f0-2d7369b0ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "\n",
    "    # Ensure deterministic behavior (slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b18ded-6904-4b6a-948e-8da14154e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(dataset[\"validation\"], batch_size=16)\n",
    "# print(len(X))\n",
    "# print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4226c616-6591-45d4-82c2-90b7ad490845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=3):  # e.g., 3-class task\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        # self.h1 = nn.Linear(self.bert.config.hidden_size, num_labels * 12)\n",
    "        # self.h1a = \n",
    "        # self.classifier = nn.Linear(num_labels * 12, num_labels)\n",
    "        # print(num_labels)\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.3),    # optional\n",
    "                # nn.Linear(self.bert.config.hidden_size, num_labels * 12),\n",
    "                # nn.GELU(),          # <-- activation\n",
    "                # nn.Dropout(0.3),    # optional\n",
    "                # nn.Linear(num_labels * 12, num_labels)\n",
    "                nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        cls_output = outputs.last_hidden_state[:,0,:]  # CLS token\n",
    "        # x = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08048ce1-8224-440f-88cf-38a52e079ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = [\"I love AI!\", \"This is terrible...\"]\n",
    "# labels = torch.tensor([1, 0])  # example labels\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "model = CustomBertClassifier(num_labels=5)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d138ff5-acc0-40ca-8652-dad662e7336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=120)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab647866-d98a-46c7-8f99-cdebc217290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0747ed-1f3d-45fd-884e-d7715ff1e8d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X = []\n",
    "# y = []\n",
    "# for n in dataset[\"train\"]:\n",
    "#     x = tokenizer(n[\"sentence\"], return_tensors='pt', padding=True, truncation=True)\n",
    "#     labels = [0, 0, 0]\n",
    "#     labels[n[\"label\"]] = 1\n",
    "#     x[\"label\"] = labels\n",
    "#     X.append(x)\n",
    "#     # y.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74996e38-cf4b-4294-b1a0-03911a03ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "def to_device(batch):\n",
    "    return {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "    \n",
    "datasetMap = dataset\n",
    "# datasetMap = datasetMap.map(to_device, batched=True, batch_size=len(dataset[\"train\"]))\n",
    "datasetMap = datasetMap.map(tokenize, batched=True)\n",
    "datasetMap = datasetMap.rename_column(\"label\", \"labels\")  # rename for HF models\n",
    "datasetMap.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57de9d1b-ca7d-4bc5-a9cd-18063fc74225",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_loader = DataLoader(datasetMap[\"train\"], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(datasetMap[\"validation\"], batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961a7ef-a9d3-4fbd-9429-551a3c0e50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.tensor(X)\n",
    "# y = torch.tensor(y)\n",
    "# batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     # TensorDataset(X, y),\n",
    "#     X,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c30336af-3ebd-4059-81a0-7d63a75dfeb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 320, Loss: 0.049106602743268016, correctP: 0.984375\n",
      "Step 640, Loss: 0.050870767375454304, correctP: 0.9828125238418579\n",
      "Step 960, Loss: 0.04840591934820016, correctP: 0.98333340883255\n",
      "Step 1280, Loss: 0.05806019314331934, correctP: 0.98046875\n",
      "Step 1600, Loss: 0.05887166610918939, correctP: 0.9799999594688416\n",
      "Step 1920, Loss: 0.07467282174620778, correctP: 0.9750000238418579\n",
      "Step 2240, Loss: 0.07935777713012482, correctP: 0.9723214507102966\n",
      "Step 2560, Loss: 0.08092421702458523, correctP: 0.9710937738418579\n",
      "Step 2880, Loss: 0.0808781619131979, correctP: 0.9722222685813904\n",
      "Step 3200, Loss: 0.07890538932289928, correctP: 0.9724999666213989\n",
      "Step 3520, Loss: 0.08102125272747468, correctP: 0.9715908765792847\n",
      "Step 3840, Loss: 0.07991344585704306, correctP: 0.9721354842185974\n",
      "Step 4160, Loss: 0.07789861602135575, correctP: 0.9728365540504456\n",
      "Step 4480, Loss: 0.07721087905977453, correctP: 0.9732142686843872\n",
      "Step 4800, Loss: 0.07636221000303825, correctP: 0.9733333587646484\n",
      "Step 5120, Loss: 0.07679128256859258, correctP: 0.9736328125\n",
      "Step 5440, Loss: 0.07696712782935185, correctP: 0.9735294580459595\n",
      "Step 5760, Loss: 0.07670229755652448, correctP: 0.9737847447395325\n",
      "Step 6080, Loss: 0.07559621676214431, correctP: 0.9741776585578918\n",
      "Step 6400, Loss: 0.07362970719579608, correctP: 0.9748437404632568\n",
      "Step 6720, Loss: 0.07329990771111278, correctP: 0.9750000238418579\n",
      "Step 7040, Loss: 0.07324548148634759, correctP: 0.9751420021057129\n",
      "Step 7360, Loss: 0.07189171034437807, correctP: 0.9756792783737183\n",
      "Step 7680, Loss: 0.07165600026686055, correctP: 0.97552090883255\n",
      "Step 8000, Loss: 0.07160489848442375, correctP: 0.9752500653266907\n",
      "Step 8320, Loss: 0.07243459332340325, correctP: 0.9750000238418579\n",
      "Step 320, Loss: 2.3977669954299925, correctP: 0.5375000238418579\n",
      "Step 640, Loss: 2.5807332038879394, correctP: 0.5093750357627869\n",
      "Step 960, Loss: 2.6460747400919598, correctP: 0.5052083730697632\n",
      "Train Epoch 1, Loss: 0.0730, correctP: 0.9750702381134033\n",
      "Val Epoch 1, Loss: 2.6754, correctP: 0.5022706985473633\n",
      "Step 320, Loss: 0.07045830702409148, correctP: 0.981249988079071\n",
      "Step 640, Loss: 0.05136004430241883, correctP: 0.984375\n",
      "Step 960, Loss: 0.04989059800282121, correctP: 0.9822916984558105\n",
      "Step 1280, Loss: 0.052049145987257364, correctP: 0.981249988079071\n",
      "Step 1600, Loss: 0.050105568850412965, correctP: 0.9824999570846558\n",
      "Step 1920, Loss: 0.048262446909211575, correctP: 0.9838542342185974\n",
      "Step 2240, Loss: 0.04830329876526126, correctP: 0.9839285612106323\n",
      "Step 2560, Loss: 0.04815199901931919, correctP: 0.984375\n",
      "Step 2880, Loss: 0.04638298902557128, correctP: 0.984375\n",
      "Step 3200, Loss: 0.04745656034909189, correctP: 0.984375\n",
      "Step 3520, Loss: 0.048207403014583344, correctP: 0.9835227131843567\n",
      "Step 3840, Loss: 0.04939521813260702, correctP: 0.9835938215255737\n",
      "Step 4160, Loss: 0.05000444454176781, correctP: 0.9836538434028625\n",
      "Step 4480, Loss: 0.050374614592042885, correctP: 0.9837053418159485\n",
      "Step 4800, Loss: 0.049430773930313684, correctP: 0.98416668176651\n",
      "Step 5120, Loss: 0.049129122159502006, correctP: 0.9839844107627869\n",
      "Step 5440, Loss: 0.05087096506647547, correctP: 0.9834558963775635\n",
      "Step 5760, Loss: 0.05141969931234295, correctP: 0.9833333492279053\n",
      "Step 6080, Loss: 0.05099599027246433, correctP: 0.9833881855010986\n",
      "Step 6400, Loss: 0.05210270872688852, correctP: 0.9829687476158142\n",
      "Step 6720, Loss: 0.05242157562391921, correctP: 0.9827381372451782\n",
      "Step 7040, Loss: 0.05242810143813999, correctP: 0.9830965399742126\n",
      "Step 7360, Loss: 0.052505683691402814, correctP: 0.9834238886833191\n",
      "Step 7680, Loss: 0.05170886492026815, correctP: 0.9834635853767395\n",
      "Step 8000, Loss: 0.05188827496673912, correctP: 0.983500063419342\n",
      "Step 8320, Loss: 0.05184603579825937, correctP: 0.9836538434028625\n",
      "Step 320, Loss: 2.5047529578208922, correctP: 0.550000011920929\n",
      "Step 640, Loss: 2.728063243627548, correctP: 0.5093750357627869\n",
      "Step 960, Loss: 2.7768606781959533, correctP: 0.5072916746139526\n",
      "Train Epoch 2, Loss: 0.0526, correctP: 0.9832631349563599\n",
      "Val Epoch 2, Loss: 2.8044, correctP: 0.5022706985473633\n",
      "Step 320, Loss: 0.04027558867819607, correctP: 0.9906250238418579\n",
      "Step 640, Loss: 0.039644341845996675, correctP: 0.9906250238418579\n",
      "Step 960, Loss: 0.039304173551499844, correctP: 0.9895833730697632\n",
      "Step 1280, Loss: 0.043768042628653345, correctP: 0.9867187738418579\n",
      "Step 1600, Loss: 0.042619459237903355, correctP: 0.9868749976158142\n",
      "Step 1920, Loss: 0.04547553278195361, correctP: 0.9859375357627869\n",
      "Step 2240, Loss: 0.04525845051476998, correctP: 0.987500011920929\n",
      "Step 2560, Loss: 0.05157860577455722, correctP: 0.984375\n",
      "Step 2880, Loss: 0.05156690993139313, correctP: 0.984375\n",
      "Step 3200, Loss: 0.04980212671682239, correctP: 0.98499995470047\n",
      "Step 3520, Loss: 0.05045194139873439, correctP: 0.9840908646583557\n",
      "Step 3840, Loss: 0.05089552964782342, correctP: 0.9835938215255737\n",
      "Step 4160, Loss: 0.05279575603154416, correctP: 0.9824519157409668\n",
      "Step 4480, Loss: 0.05356753264620368, correctP: 0.9821428656578064\n",
      "Step 4800, Loss: 0.05268627492400507, correctP: 0.9822916984558105\n",
      "Step 5120, Loss: 0.051245518118957986, correctP: 0.9830078482627869\n",
      "Step 5440, Loss: 0.050933742986115464, correctP: 0.9830882549285889\n",
      "Step 5760, Loss: 0.05083688136138436, correctP: 0.9831597208976746\n",
      "Step 6080, Loss: 0.0502038166515137, correctP: 0.9833881855010986\n",
      "Step 6400, Loss: 0.05020263011334464, correctP: 0.9832812547683716\n",
      "Step 6720, Loss: 0.05125711680656033, correctP: 0.9827381372451782\n",
      "Step 7040, Loss: 0.051413348386995496, correctP: 0.9825283885002136\n",
      "Step 7360, Loss: 0.051386680393277304, correctP: 0.9828804135322571\n",
      "Step 7680, Loss: 0.05344533931929618, correctP: 0.9828125238418579\n",
      "Step 8000, Loss: 0.05262100520730019, correctP: 0.9830000400543213\n",
      "Step 8320, Loss: 0.05191140041256753, correctP: 0.9830529093742371\n",
      "Step 320, Loss: 2.5716429948806763, correctP: 0.543749988079071\n",
      "Step 640, Loss: 2.7358975768089295, correctP: 0.518750011920929\n",
      "Step 960, Loss: 2.8218587080637616, correctP: 0.515625\n",
      "Train Epoch 3, Loss: 0.0518, correctP: 0.9830290079116821\n",
      "Val Epoch 3, Loss: 2.8435, correctP: 0.5122615694999695\n",
      "Step 320, Loss: 0.046860738378018144, correctP: 0.9781250357627869\n",
      "Step 640, Loss: 0.04329560080077499, correctP: 0.981249988079071\n",
      "Step 960, Loss: 0.04576929874407749, correctP: 0.98333340883255\n",
      "Step 1280, Loss: 0.04508833215804771, correctP: 0.9828125238418579\n",
      "Step 1600, Loss: 0.05123979972675443, correctP: 0.9818750023841858\n",
      "Step 1920, Loss: 0.05279072035724918, correctP: 0.9812500476837158\n",
      "Step 2240, Loss: 0.058080277498811485, correctP: 0.9808035492897034\n",
      "Step 2560, Loss: 0.05820815973856952, correctP: 0.9808593988418579\n",
      "Step 2880, Loss: 0.05553808149933401, correctP: 0.9819444417953491\n",
      "Step 3200, Loss: 0.055008814616594466, correctP: 0.9821874499320984\n",
      "Step 3520, Loss: 0.05496392165607011, correctP: 0.9818181395530701\n",
      "Step 3840, Loss: 0.05493668917139682, correctP: 0.9820312857627869\n",
      "Step 4160, Loss: 0.05455822841658329, correctP: 0.982692301273346\n",
      "Step 4480, Loss: 0.05331870971214292, correctP: 0.9832589626312256\n",
      "Step 4800, Loss: 0.0520366645247365, correctP: 0.9835416674613953\n",
      "Step 5120, Loss: 0.05123136728216195, correctP: 0.984179675579071\n",
      "Step 5440, Loss: 0.051615582922857034, correctP: 0.9838235378265381\n",
      "Step 5760, Loss: 0.051028088048203954, correctP: 0.9840278029441833\n",
      "Step 6080, Loss: 0.051002219267875744, correctP: 0.9838815927505493\n",
      "Step 6400, Loss: 0.052078086480032655, correctP: 0.9835937023162842\n",
      "Step 6720, Loss: 0.0530937511274325, correctP: 0.9833333492279053\n",
      "Step 7040, Loss: 0.0533956545214592, correctP: 0.9833806753158569\n",
      "Step 7360, Loss: 0.05325695286180986, correctP: 0.9832879900932312\n",
      "Step 7680, Loss: 0.0529374846315477, correctP: 0.9834635853767395\n",
      "Step 8000, Loss: 0.0515815165694803, correctP: 0.984000027179718\n",
      "Step 8320, Loss: 0.051670182222285525, correctP: 0.9837740659713745\n",
      "Step 320, Loss: 2.656457471847534, correctP: 0.5375000238418579\n",
      "Step 640, Loss: 2.825816643238068, correctP: 0.512499988079071\n",
      "Step 960, Loss: 2.8826014439264935, correctP: 0.5\n",
      "Train Epoch 4, Loss: 0.0512, correctP: 0.9838483333587646\n",
      "Val Epoch 4, Loss: 2.9242, correctP: 0.4950045645236969\n",
      "Step 320, Loss: 0.03617654941044748, correctP: 0.987500011920929\n",
      "Step 640, Loss: 0.038601406780071554, correctP: 0.987500011920929\n",
      "Step 960, Loss: 0.04710517738324901, correctP: 0.98333340883255\n",
      "Step 1280, Loss: 0.04165008140262216, correctP: 0.984375\n",
      "Step 1600, Loss: 0.05045438783243299, correctP: 0.981249988079071\n",
      "Step 1920, Loss: 0.04780763648450374, correctP: 0.9822916984558105\n",
      "Step 2240, Loss: 0.04505273333218481, correctP: 0.9834821224212646\n",
      "Step 2560, Loss: 0.04241249032202177, correctP: 0.984375\n",
      "Step 2880, Loss: 0.04306763305535747, correctP: 0.9833333492279053\n",
      "Step 3200, Loss: 0.040899140294641256, correctP: 0.984375\n",
      "Step 3520, Loss: 0.040209350641816854, correctP: 0.9846590757369995\n",
      "Step 3840, Loss: 0.039311997532301274, correctP: 0.9846354722976685\n",
      "Step 4160, Loss: 0.03795878920310105, correctP: 0.9853365421295166\n",
      "Step 4480, Loss: 0.03827226927470682, correctP: 0.9852678775787354\n",
      "Step 4800, Loss: 0.03842717507388443, correctP: 0.9850000143051147\n",
      "Step 5120, Loss: 0.039487669528170954, correctP: 0.984570324420929\n",
      "Step 5440, Loss: 0.03961504235836294, correctP: 0.984375\n",
      "Step 5760, Loss: 0.041090116551559835, correctP: 0.983506977558136\n",
      "Step 6080, Loss: 0.04116472966086707, correctP: 0.9835526943206787\n",
      "Step 6400, Loss: 0.0413715906208381, correctP: 0.9835937023162842\n",
      "Step 6720, Loss: 0.042362805764146506, correctP: 0.9833333492279053\n",
      "Step 7040, Loss: 0.04279268048068678, correctP: 0.9830965399742126\n",
      "Step 7360, Loss: 0.042365127945642754, correctP: 0.9835597276687622\n",
      "Step 7680, Loss: 0.041508288402110335, correctP: 0.9839844107627869\n",
      "Step 8000, Loss: 0.0408663447983563, correctP: 0.984000027179718\n",
      "Step 8320, Loss: 0.040866218999816246, correctP: 0.9840144515037537\n",
      "Step 320, Loss: 2.6446513652801515, correctP: 0.5250000357627869\n",
      "Step 640, Loss: 2.807315492630005, correctP: 0.4906249940395355\n",
      "Step 960, Loss: 2.8822287638982136, correctP: 0.48645836114883423\n",
      "Train Epoch 5, Loss: 0.0411, correctP: 0.9840824007987976\n",
      "Val Epoch 5, Loss: 2.9264, correctP: 0.48319709300994873\n",
      "Step 320, Loss: 0.054663475742563604, correctP: 0.971875011920929\n",
      "Step 640, Loss: 0.053348264074884355, correctP: 0.9781250357627869\n",
      "Step 960, Loss: 0.04553113008538882, correctP: 0.98333340883255\n",
      "Step 1280, Loss: 0.047673599619884044, correctP: 0.984375\n",
      "Step 1600, Loss: 0.04957813412882388, correctP: 0.98499995470047\n",
      "Step 1920, Loss: 0.048239319251539806, correctP: 0.9854167103767395\n",
      "Step 2240, Loss: 0.04605815416933703, correctP: 0.9866071343421936\n",
      "Step 2560, Loss: 0.04505592993809841, correctP: 0.987109363079071\n",
      "Step 2880, Loss: 0.046293059746838276, correctP: 0.9868055582046509\n",
      "Step 3200, Loss: 0.04517267286777496, correctP: 0.9865624904632568\n",
      "Step 3520, Loss: 0.04814452950165353, correctP: 0.984943151473999\n",
      "Step 3840, Loss: 0.05045348253333941, correctP: 0.9835938215255737\n",
      "Step 4160, Loss: 0.049368355921111426, correctP: 0.9836538434028625\n",
      "Step 4480, Loss: 0.049521173506842125, correctP: 0.9837053418159485\n",
      "Step 4800, Loss: 0.04787409937319656, correctP: 0.984375\n",
      "Step 5120, Loss: 0.04686042754183291, correctP: 0.984570324420929\n",
      "Step 5440, Loss: 0.046907356999102326, correctP: 0.9841911792755127\n",
      "Step 5760, Loss: 0.04738678826510699, correctP: 0.9840278029441833\n",
      "Step 6080, Loss: 0.04938181489583497, correctP: 0.9833881855010986\n",
      "Step 6400, Loss: 0.04893369766301475, correctP: 0.9835937023162842\n",
      "Step 6720, Loss: 0.049096458523889025, correctP: 0.9833333492279053\n",
      "Step 7040, Loss: 0.04811962826325643, correctP: 0.9838067889213562\n",
      "Step 7360, Loss: 0.048003076984668554, correctP: 0.9836956262588501\n",
      "Step 7680, Loss: 0.04782874921802431, correctP: 0.9838542342185974\n",
      "Step 8000, Loss: 0.04719845796562731, correctP: 0.983875036239624\n",
      "Step 8320, Loss: 0.0470554516149255, correctP: 0.9838942289352417\n",
      "Step 320, Loss: 2.491860198974609, correctP: 0.5562500357627869\n",
      "Step 640, Loss: 2.7542286276817323, correctP: 0.5250000357627869\n",
      "Step 960, Loss: 2.862963604927063, correctP: 0.518750011920929\n",
      "Train Epoch 6, Loss: 0.0481, correctP: 0.9838483333587646\n",
      "Val Epoch 6, Loss: 2.9162, correctP: 0.5059037208557129\n",
      "Step 320, Loss: 0.03599825003184378, correctP: 0.984375\n",
      "Step 640, Loss: 0.033004842489026485, correctP: 0.987500011920929\n",
      "Step 960, Loss: 0.035241023637354374, correctP: 0.9854167103767395\n",
      "Step 1280, Loss: 0.04247884708456695, correctP: 0.983593761920929\n",
      "Step 1600, Loss: 0.0415146673657, correctP: 0.9837499856948853\n",
      "Step 1920, Loss: 0.04116650591992463, correctP: 0.9843750596046448\n",
      "Step 2240, Loss: 0.039447430507945164, correctP: 0.9852678775787354\n",
      "Step 2560, Loss: 0.04085972171742469, correctP: 0.984375\n",
      "Step 2880, Loss: 0.04074358814137263, correctP: 0.9840278029441833\n",
      "Step 3200, Loss: 0.04084952066885308, correctP: 0.9840624928474426\n",
      "Step 3520, Loss: 0.03952223192477091, correctP: 0.9843749403953552\n",
      "Step 3840, Loss: 0.04079462136530007, correctP: 0.9841146469116211\n",
      "Step 4160, Loss: 0.041088252876383756, correctP: 0.9838942289352417\n",
      "Step 4480, Loss: 0.0416453111118504, correctP: 0.9834821224212646\n",
      "Step 4800, Loss: 0.04406306988870104, correctP: 0.9831250309944153\n",
      "Step 5120, Loss: 0.04393025607860181, correctP: 0.983203113079071\n",
      "Step 5440, Loss: 0.04416761304361417, correctP: 0.9834558963775635\n",
      "Step 5760, Loss: 0.04507065569826712, correctP: 0.9836806058883667\n",
      "Step 6080, Loss: 0.04446797262583124, correctP: 0.983717143535614\n",
      "Step 6400, Loss: 0.04402341666631401, correctP: 0.9840624928474426\n",
      "Step 6720, Loss: 0.04432835728373556, correctP: 0.983779788017273\n",
      "Step 7040, Loss: 0.044779152569191696, correctP: 0.9835227131843567\n",
      "Step 7360, Loss: 0.045025733560728635, correctP: 0.9835597276687622\n",
      "Step 7680, Loss: 0.04449313403553485, correctP: 0.9841146469116211\n",
      "Step 8000, Loss: 0.04369663429353386, correctP: 0.9845000505447388\n",
      "Step 8320, Loss: 0.04315170173694451, correctP: 0.9848557710647583\n",
      "Step 320, Loss: 2.6566264390945435, correctP: 0.5531250238418579\n",
      "Step 640, Loss: 2.9314804911613463, correctP: 0.5218750238418579\n",
      "Step 960, Loss: 3.003151551882426, correctP: 0.5093750357627869\n",
      "Train Epoch 7, Loss: 0.0430, correctP: 0.9849016666412354\n",
      "Val Epoch 7, Loss: 3.0395, correctP: 0.5022706985473633\n",
      "Step 320, Loss: 0.05508987260982394, correctP: 0.971875011920929\n",
      "Step 640, Loss: 0.045680705283302814, correctP: 0.9781250357627869\n",
      "Step 960, Loss: 0.04864591261527191, correctP: 0.9770833849906921\n",
      "Step 1280, Loss: 0.04730695537291467, correctP: 0.98046875\n",
      "Step 1600, Loss: 0.05067600693553686, correctP: 0.9806249737739563\n",
      "Step 1920, Loss: 0.04905620175413787, correctP: 0.9812500476837158\n",
      "Step 2240, Loss: 0.04380948496982455, correctP: 0.9839285612106323\n",
      "Step 2560, Loss: 0.044402407418238, correctP: 0.983593761920929\n",
      "Step 2880, Loss: 0.043661070938429074, correctP: 0.9836806058883667\n",
      "Step 3200, Loss: 0.04057409801054746, correctP: 0.9853124618530273\n",
      "Step 3520, Loss: 0.039056828753514725, correctP: 0.985511302947998\n",
      "Step 3840, Loss: 0.03868098910509919, correctP: 0.9856771230697632\n",
      "Step 4160, Loss: 0.03930694891605526, correctP: 0.986057698726654\n",
      "Step 4480, Loss: 0.03828825597302057, correctP: 0.9866071343421936\n",
      "Step 4800, Loss: 0.0371690453379415, correctP: 0.9868749976158142\n",
      "Step 5120, Loss: 0.03777369712406653, correctP: 0.986523449420929\n",
      "Step 5440, Loss: 0.03789487174807993, correctP: 0.9867647290229797\n",
      "Step 5760, Loss: 0.03668896615595764, correctP: 0.9869791865348816\n",
      "Step 6080, Loss: 0.03599422899454734, correctP: 0.9873355627059937\n",
      "Step 6400, Loss: 0.03589388811669778, correctP: 0.9874999523162842\n",
      "Step 6720, Loss: 0.03607857311310779, correctP: 0.987500011920929\n",
      "Step 7040, Loss: 0.03559906415757723, correctP: 0.9877840876579285\n",
      "Step 7360, Loss: 0.03541967670139654, correctP: 0.9879075884819031\n",
      "Step 7680, Loss: 0.03674044563619342, correctP: 0.9875000715255737\n",
      "Step 8000, Loss: 0.03732795704109594, correctP: 0.987125039100647\n",
      "Step 8320, Loss: 0.03744083110538598, correctP: 0.9872596263885498\n",
      "Step 320, Loss: 2.645895838737488, correctP: 0.5375000238418579\n",
      "Step 640, Loss: 2.885110592842102, correctP: 0.5\n",
      "Step 960, Loss: 2.9497499068578086, correctP: 0.4989583492279053\n",
      "Train Epoch 8, Loss: 0.0374, correctP: 0.9872425198554993\n",
      "Val Epoch 8, Loss: 3.0267, correctP: 0.4859218895435333\n",
      "Step 320, Loss: 0.017293108534067868, correctP: 0.9937500357627869\n",
      "Step 640, Loss: 0.0170940785552375, correctP: 0.995312511920929\n",
      "Step 960, Loss: 0.016103745282938082, correctP: 0.9958333969116211\n",
      "Step 1280, Loss: 0.018941105325939133, correctP: 0.9945312738418579\n",
      "Step 1600, Loss: 0.020161497225053608, correctP: 0.9943749904632568\n",
      "Step 1920, Loss: 0.02098625615471974, correctP: 0.9937500357627869\n",
      "Step 2240, Loss: 0.02232887434906193, correctP: 0.9933035969734192\n",
      "Step 2560, Loss: 0.022730543644865975, correctP: 0.993359386920929\n",
      "Step 2880, Loss: 0.0227261983944724, correctP: 0.9934027791023254\n",
      "Step 3200, Loss: 0.023312040283344687, correctP: 0.9931249618530273\n",
      "Step 3520, Loss: 0.024953098662874915, correctP: 0.9923295378684998\n",
      "Step 3840, Loss: 0.025840566807892172, correctP: 0.9921875596046448\n",
      "Step 4160, Loss: 0.026028607019151634, correctP: 0.9918269515037537\n",
      "Step 4480, Loss: 0.027217390717539404, correctP: 0.9912946224212646\n",
      "Step 4800, Loss: 0.027430638687995574, correctP: 0.9912500381469727\n",
      "Step 5120, Loss: 0.027023672264476772, correctP: 0.9912109375\n",
      "Step 5440, Loss: 0.02756856821121319, correctP: 0.9911764860153198\n",
      "Step 5760, Loss: 0.02776268727741101, correctP: 0.9907986521720886\n",
      "Step 6080, Loss: 0.027273847367369423, correctP: 0.9909539818763733\n",
      "Step 6400, Loss: 0.02806506070890464, correctP: 0.9907812476158142\n",
      "Step 6720, Loss: 0.028278040663073104, correctP: 0.9909226298332214\n",
      "Step 7040, Loss: 0.028614105890632013, correctP: 0.9910510778427124\n",
      "Step 7360, Loss: 0.029065158401372963, correctP: 0.9911684393882751\n",
      "Step 7680, Loss: 0.029909677559044213, correctP: 0.99114590883255\n",
      "Step 8000, Loss: 0.029960108718369155, correctP: 0.9911250472068787\n",
      "Step 8320, Loss: 0.030001409635252246, correctP: 0.9911057949066162\n",
      "Step 320, Loss: 2.6422795057296753, correctP: 0.546875\n",
      "Step 640, Loss: 2.877109181880951, correctP: 0.510937511920929\n",
      "Step 960, Loss: 2.9834359884262085, correctP: 0.503125011920929\n",
      "Train Epoch 9, Loss: 0.0300, correctP: 0.9911049008369446\n",
      "Val Epoch 9, Loss: 3.0309, correctP: 0.4968210756778717\n",
      "Step 320, Loss: 0.017452902859076858, correctP: 0.9906250238418579\n",
      "Step 640, Loss: 0.028419128071982415, correctP: 0.989062488079071\n",
      "Step 960, Loss: 0.02523521943949163, correctP: 0.9895833730697632\n",
      "Step 1280, Loss: 0.020742702460847796, correctP: 0.9921875\n",
      "Step 1600, Loss: 0.019059438109397888, correctP: 0.9931249618530273\n",
      "Step 1920, Loss: 0.018947925643684964, correctP: 0.9937500357627869\n",
      "Step 2240, Loss: 0.019586687323836876, correctP: 0.9933035969734192\n",
      "Step 2560, Loss: 0.020556098174711223, correctP: 0.9921875\n",
      "Step 2880, Loss: 0.024198619158576348, correctP: 0.9916666746139526\n",
      "Step 3200, Loss: 0.024471229474293067, correctP: 0.9915624856948853\n",
      "Step 3520, Loss: 0.02491315159078857, correctP: 0.9911931753158569\n",
      "Step 3840, Loss: 0.029280476434117495, correctP: 0.9906250238418579\n",
      "Step 4160, Loss: 0.031722725335902605, correctP: 0.9896634817123413\n",
      "Step 4480, Loss: 0.032016115608192716, correctP: 0.989062488079071\n",
      "Step 4800, Loss: 0.031245772279022883, correctP: 0.9893749952316284\n",
      "Step 5120, Loss: 0.029834716142067917, correctP: 0.990039050579071\n",
      "Step 5440, Loss: 0.02911690063706106, correctP: 0.9902573823928833\n",
      "Step 5760, Loss: 0.030387177757479043, correctP: 0.9897569417953491\n",
      "Step 6080, Loss: 0.030596479650366267, correctP: 0.9898026585578918\n",
      "Step 6400, Loss: 0.029687279610079713, correctP: 0.9903124570846558\n",
      "Step 6720, Loss: 0.0310986694408625, correctP: 0.9898809790611267\n",
      "Step 7040, Loss: 0.03180531866964884, correctP: 0.9897726774215698\n",
      "Step 7360, Loss: 0.03148947598604971, correctP: 0.9899455904960632\n",
      "Step 7680, Loss: 0.03178795172473959, correctP: 0.9898437857627869\n",
      "Step 8000, Loss: 0.031659534021746365, correctP: 0.9897500276565552\n",
      "Step 8320, Loss: 0.03128033795152218, correctP: 0.9897836446762085\n",
      "Step 320, Loss: 2.7077004313468933, correctP: 0.5406250357627869\n",
      "Step 640, Loss: 2.9218877494335174, correctP: 0.5140625238418579\n",
      "Step 960, Loss: 2.984395364920298, correctP: 0.5072916746139526\n",
      "Train Epoch 10, Loss: 0.0314, correctP: 0.9897003769874573\n",
      "Val Epoch 10, Loss: 3.0494, correctP: 0.4968210756778717\n",
      "Step 320, Loss: 0.026886526215821504, correctP: 0.9937500357627869\n",
      "Step 640, Loss: 0.021406251518055797, correctP: 0.9937500357627869\n",
      "Step 960, Loss: 0.019317108408237495, correctP: 0.9927083849906921\n",
      "Step 1280, Loss: 0.018999033415457233, correctP: 0.992968738079071\n",
      "Step 1600, Loss: 0.019372834395617246, correctP: 0.9931249618530273\n",
      "Step 1920, Loss: 0.018689519520072886, correctP: 0.9932292103767395\n",
      "Step 2240, Loss: 0.018840334438053625, correctP: 0.9933035969734192\n",
      "Step 2560, Loss: 0.01890321177779697, correctP: 0.9925781488418579\n",
      "Step 2880, Loss: 0.020586567923116188, correctP: 0.9923611283302307\n",
      "Step 3200, Loss: 0.020972916036844255, correctP: 0.9925000071525574\n",
      "Step 3520, Loss: 0.02188700385688042, correctP: 0.9920454025268555\n",
      "Step 3840, Loss: 0.02386344932601787, correctP: 0.9916667342185974\n",
      "Step 4160, Loss: 0.025247397766305277, correctP: 0.9913461804389954\n",
      "Step 4480, Loss: 0.025757967921838695, correctP: 0.9910714626312256\n",
      "Step 4800, Loss: 0.02502004565205425, correctP: 0.9912500381469727\n",
      "Step 5120, Loss: 0.0247147701040376, correctP: 0.9912109375\n",
      "Step 5440, Loss: 0.024644240883507713, correctP: 0.9911764860153198\n",
      "Step 5760, Loss: 0.025236697817712817, correctP: 0.9907986521720886\n",
      "Step 6080, Loss: 0.02524556091643478, correctP: 0.990789532661438\n",
      "Step 6400, Loss: 0.02513614297728054, correctP: 0.9907812476158142\n",
      "Step 6720, Loss: 0.025744286282653257, correctP: 0.9906250238418579\n",
      "Step 7040, Loss: 0.026130763559856197, correctP: 0.9904829263687134\n",
      "Step 7360, Loss: 0.02612002741585931, correctP: 0.9904890656471252\n",
      "Step 7680, Loss: 0.026626739462759966, correctP: 0.9903646111488342\n",
      "Step 8000, Loss: 0.027840909864753485, correctP: 0.9901250600814819\n",
      "Step 8320, Loss: 0.02801143676490308, correctP: 0.9900240302085876\n",
      "Step 320, Loss: 2.7048236608505247, correctP: 0.5531250238418579\n",
      "Step 640, Loss: 2.9913206815719606, correctP: 0.5062500238418579\n",
      "Step 960, Loss: 3.0611005544662477, correctP: 0.5062500238418579\n",
      "Train Epoch 11, Loss: 0.0274, correctP: 0.9902855753898621\n",
      "Val Epoch 11, Loss: 3.1401, correctP: 0.4986376166343689\n",
      "Step 320, Loss: 0.030814793438185006, correctP: 0.987500011920929\n",
      "Step 640, Loss: 0.028902460366953164, correctP: 0.9906250238418579\n",
      "Step 960, Loss: 0.02389736014495914, correctP: 0.9927083849906921\n",
      "Step 1280, Loss: 0.02160711002361495, correctP: 0.9937500357627869\n",
      "Step 1600, Loss: 0.02474461822072044, correctP: 0.9912499785423279\n",
      "Step 1920, Loss: 0.025286113837501033, correctP: 0.99114590883255\n",
      "Step 2240, Loss: 0.023390574116326335, correctP: 0.9924107193946838\n",
      "Step 2560, Loss: 0.022566617031407075, correctP: 0.9925781488418579\n",
      "Step 2880, Loss: 0.021416852642626813, correctP: 0.9927083849906921\n",
      "Step 3200, Loss: 0.021030698287067936, correctP: 0.9931249618530273\n",
      "Step 3520, Loss: 0.02183677612126551, correctP: 0.9928976893424988\n",
      "Step 3840, Loss: 0.020834165842582783, correctP: 0.9932292103767395\n",
      "Step 4160, Loss: 0.020546687963472393, correctP: 0.9935095906257629\n",
      "Step 4480, Loss: 0.020397512013109267, correctP: 0.9930803775787354\n",
      "Step 4800, Loss: 0.02193249306680324, correctP: 0.9927083849906921\n",
      "Step 5120, Loss: 0.021496459255649823, correctP: 0.992968738079071\n",
      "Step 5440, Loss: 0.023019850275629913, correctP: 0.9926470518112183\n",
      "Step 5760, Loss: 0.024561984849020113, correctP: 0.9921875\n",
      "Step 6080, Loss: 0.02411106089283222, correctP: 0.9924342632293701\n",
      "Step 6400, Loss: 0.0243243350010016, correctP: 0.9925000071525574\n",
      "Step 6720, Loss: 0.023868787974130272, correctP: 0.992559552192688\n",
      "Step 7040, Loss: 0.02388389589640693, correctP: 0.9923295378684998\n",
      "Step 7360, Loss: 0.02395385685756438, correctP: 0.9923912882804871\n",
      "Step 7680, Loss: 0.02358213130379833, correctP: 0.9924479722976685\n",
      "Step 8000, Loss: 0.023897531944094226, correctP: 0.9922500252723694\n",
      "Step 8320, Loss: 0.02420863183640624, correctP: 0.9919471144676208\n",
      "Step 320, Loss: 2.667590117454529, correctP: 0.5562500357627869\n",
      "Step 640, Loss: 2.9193212270736693, correctP: 0.5171875357627869\n",
      "Step 960, Loss: 3.0151834726333617, correctP: 0.5072916746139526\n",
      "Train Epoch 12, Loss: 0.0245, correctP: 0.9919241666793823\n",
      "Val Epoch 12, Loss: 3.0721, correctP: 0.5004541277885437\n",
      "Step 320, Loss: 0.01467204277869314, correctP: 0.9937500357627869\n",
      "Step 640, Loss: 0.016990488092415035, correctP: 0.9937500357627869\n",
      "Step 960, Loss: 0.016437483951449394, correctP: 0.9947917461395264\n",
      "Step 1280, Loss: 0.0166743733221665, correctP: 0.9945312738418579\n",
      "Step 1600, Loss: 0.014607509430497885, correctP: 0.9956249594688416\n",
      "Step 1920, Loss: 0.013797235997238507, correctP: 0.9963542222976685\n",
      "Step 2240, Loss: 0.012866698742644594, correctP: 0.9964285492897034\n",
      "Step 2560, Loss: 0.011930702730023768, correctP: 0.996874988079071\n",
      "Step 2880, Loss: 0.012236850173212587, correctP: 0.9961805939674377\n",
      "Step 3200, Loss: 0.011502078893827275, correctP: 0.9965624809265137\n",
      "Step 3520, Loss: 0.013695970604153857, correctP: 0.9957386255264282\n",
      "Step 3840, Loss: 0.014640887196098145, correctP: 0.9953125715255737\n",
      "Step 4160, Loss: 0.014129669062542514, correctP: 0.9954326748847961\n",
      "Step 4480, Loss: 0.014759433356812224, correctP: 0.9950892925262451\n",
      "Step 4800, Loss: 0.01422790355126684, correctP: 0.9954167008399963\n",
      "Step 5120, Loss: 0.014101217073766747, correctP: 0.9955078363418579\n",
      "Step 5440, Loss: 0.014416360162773771, correctP: 0.995036780834198\n",
      "Step 5760, Loss: 0.016137754305964334, correctP: 0.9949653148651123\n",
      "Step 6080, Loss: 0.016862696174556685, correctP: 0.994736909866333\n",
      "Step 6400, Loss: 0.01720471403794363, correctP: 0.9943749904632568\n",
      "Step 6720, Loss: 0.01854932111732307, correctP: 0.9941964745521545\n",
      "Step 7040, Loss: 0.018499280658116648, correctP: 0.9941760897636414\n",
      "Step 7360, Loss: 0.018428788220499762, correctP: 0.9942934513092041\n",
      "Step 7680, Loss: 0.018303280652374573, correctP: 0.9944010972976685\n",
      "Step 8000, Loss: 0.019062317890813574, correctP: 0.9943750500679016\n",
      "Step 8320, Loss: 0.018995664921105625, correctP: 0.9942307472229004\n",
      "Step 320, Loss: 2.8725274562835694, correctP: 0.534375011920929\n",
      "Step 640, Loss: 3.164913237094879, correctP: 0.512499988079071\n",
      "Step 960, Loss: 3.2738984346389772, correctP: 0.5072916746139526\n",
      "Train Epoch 13, Loss: 0.0188, correctP: 0.9942650198936462\n",
      "Val Epoch 13, Loss: 3.3233, correctP: 0.5004541277885437\n",
      "Step 320, Loss: 0.020010581333190204, correctP: 0.996874988079071\n",
      "Step 640, Loss: 0.018726509728003292, correctP: 0.9937500357627869\n",
      "Step 960, Loss: 0.015799974626861512, correctP: 0.9947917461395264\n",
      "Step 1280, Loss: 0.02093087543908041, correctP: 0.9921875\n",
      "Step 1600, Loss: 0.027270881989970804, correctP: 0.9899999499320984\n",
      "Step 1920, Loss: 0.02647323484222094, correctP: 0.9901041984558105\n",
      "Step 2240, Loss: 0.02589633685669729, correctP: 0.9901785850524902\n",
      "Step 2560, Loss: 0.024472569423960523, correctP: 0.990234375\n",
      "Step 2880, Loss: 0.02544131614203151, correctP: 0.9892361164093018\n",
      "Step 3200, Loss: 0.026254502692027018, correctP: 0.9893749952316284\n",
      "Step 3520, Loss: 0.027637303079774772, correctP: 0.9897726774215698\n",
      "Step 3840, Loss: 0.026529535760831398, correctP: 0.9903646111488342\n",
      "Step 4160, Loss: 0.027351553178428172, correctP: 0.9903846383094788\n",
      "Step 4480, Loss: 0.027090819425315462, correctP: 0.9906250238418579\n",
      "Step 4800, Loss: 0.02733602576190606, correctP: 0.9902083277702332\n",
      "Step 5120, Loss: 0.026673936031147604, correctP: 0.990429699420929\n",
      "Step 5440, Loss: 0.02693669785855009, correctP: 0.990073561668396\n",
      "Step 5760, Loss: 0.028100610348499484, correctP: 0.9897569417953491\n",
      "Step 6080, Loss: 0.027306669818752103, correctP: 0.9901316165924072\n",
      "Step 6400, Loss: 0.026626095571555198, correctP: 0.9903124570846558\n",
      "Step 6720, Loss: 0.02596546653824459, correctP: 0.9906250238418579\n",
      "Step 7040, Loss: 0.02580757109310732, correctP: 0.9909090399742126\n",
      "Step 7360, Loss: 0.02590727415593827, correctP: 0.9910325407981873\n",
      "Step 7680, Loss: 0.025855388523389897, correctP: 0.9910156726837158\n",
      "Step 8000, Loss: 0.025803408740554003, correctP: 0.9910000562667847\n",
      "Step 8320, Loss: 0.025452471430449247, correctP: 0.9909855723381042\n",
      "Step 320, Loss: 2.948677110671997, correctP: 0.5\n",
      "Step 640, Loss: 3.117576003074646, correctP: 0.4828124940395355\n",
      "Step 960, Loss: 3.21862743695577, correctP: 0.4802083671092987\n",
      "Train Epoch 14, Loss: 0.0262, correctP: 0.9908707737922668\n",
      "Val Epoch 14, Loss: 3.2844, correctP: 0.47138965129852295\n",
      "Step 320, Loss: 0.030719032208435238, correctP: 0.987500011920929\n",
      "Step 640, Loss: 0.020177335781045257, correctP: 0.9921875\n",
      "Step 960, Loss: 0.020585080694096782, correctP: 0.9906250238418579\n",
      "Step 1280, Loss: 0.021291748649673536, correctP: 0.9906250238418579\n",
      "Step 1600, Loss: 0.01964714529691264, correctP: 0.9912499785423279\n",
      "Step 1920, Loss: 0.020186546966821577, correctP: 0.9906250238418579\n",
      "Step 2240, Loss: 0.01944551911936807, correctP: 0.9906250238418579\n",
      "Step 2560, Loss: 0.0197306081361603, correctP: 0.991406261920929\n",
      "Step 2880, Loss: 0.018720823538023977, correctP: 0.9920139312744141\n",
      "Step 3200, Loss: 0.01795579308993183, correctP: 0.9925000071525574\n",
      "Step 3520, Loss: 0.01851072064600885, correctP: 0.9923295378684998\n",
      "Step 3840, Loss: 0.01788724658836145, correctP: 0.9927083849906921\n",
      "Step 4160, Loss: 0.016953552270737977, correctP: 0.9932692050933838\n",
      "Step 4480, Loss: 0.01829595657098772, correctP: 0.9928571581840515\n",
      "Step 4800, Loss: 0.01854001609608531, correctP: 0.9927083849906921\n",
      "Step 5120, Loss: 0.020752003511006478, correctP: 0.9921875\n",
      "Step 5440, Loss: 0.023433790676405324, correctP: 0.9913603067398071\n",
      "Step 5760, Loss: 0.02421653410161121, correctP: 0.9914931058883667\n",
      "Step 6080, Loss: 0.02358812789653281, correctP: 0.9917763471603394\n",
      "Step 6400, Loss: 0.024330487061524762, correctP: 0.9914062023162842\n",
      "Step 6720, Loss: 0.025548900135548874, correctP: 0.9912202954292297\n",
      "Step 7040, Loss: 0.02631868716879663, correctP: 0.9911931753158569\n",
      "Step 7360, Loss: 0.026250633340247947, correctP: 0.9911684393882751\n",
      "Step 7680, Loss: 0.02677557747520041, correctP: 0.9910156726837158\n",
      "Step 8000, Loss: 0.02749226978281513, correctP: 0.9908750653266907\n",
      "Step 8320, Loss: 0.027446956128831235, correctP: 0.9909855723381042\n",
      "Step 320, Loss: 2.8651068210601807, correctP: 0.528124988079071\n",
      "Step 640, Loss: 3.112935745716095, correctP: 0.512499988079071\n",
      "Step 960, Loss: 3.179626186688741, correctP: 0.5114583373069763\n",
      "Train Epoch 15, Loss: 0.0273, correctP: 0.9908707737922668\n",
      "Val Epoch 15, Loss: 3.2603, correctP: 0.5022706985473633\n"
     ]
    }
   ],
   "source": [
    "# GPU training\n",
    "step = 10\n",
    "epochs = 15\n",
    "for epoch in range(epochs):  # 3 epochs seems to be the optimal number\n",
    "    model.train()\n",
    "    total_losst = 0\n",
    "    curBatch = 0\n",
    "    correctLabelsT = 0\n",
    "    progressT = 0\n",
    "    for batch in train_loader:\n",
    "        # print(batch)\n",
    "        # break\n",
    "        optimizer.zero_grad()\n",
    "        curBatch += 1\n",
    "        # print(batch)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Count how many labels are correctly predicted in this batch\n",
    "        logitLabels = torch.argmax(logits, dim=1)\n",
    "        correctLabelsT += (logitLabels == labels).sum()\n",
    "        progressT += len(logitLabels)\n",
    "        \n",
    "        # loss = outputs.loss\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losst += loss.item()\n",
    "\n",
    "        if curBatch % step == 0:\n",
    "            print(f\"Step {progressT}, Loss: {total_losst / curBatch}, correctP: {correctLabelsT / progressT}\")\n",
    "            \n",
    "    trainLastBatch = curBatch\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        curBatch = 0\n",
    "        correctLabels = 0\n",
    "        progress = 0\n",
    "        for batch in val_loader:\n",
    "            # print(batch)\n",
    "            # break\n",
    "            optimizer.zero_grad()\n",
    "            curBatch += 1\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # print(labels)\n",
    "    \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Count how many labels are correctly predicted in this batch\n",
    "            logitLabels = torch.argmax(logits, dim=1)\n",
    "            correctLabels += (logitLabels == labels).sum()\n",
    "            progress += len(logitLabels)\n",
    "            # loss = outputs.loss\n",
    "            loss = criterion(logits, labels)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            # break\n",
    "            if curBatch % step == 0:\n",
    "                print(f\"Step {progress}, Loss: {total_loss / curBatch}, correctP: {correctLabels / progress}\")\n",
    "\n",
    "        avg_loss = total_losst / len(train_loader)\n",
    "        print(f\"Train Epoch {epoch+1}, Loss: {avg_loss:.4f}, correctP: {correctLabelsT / progressT}\")\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        print(f\"Val Epoch {epoch+1}, Loss: {avg_loss:.4f}, correctP: {correctLabels / progress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15f4c51c-1eaf-4d7e-815f-75c03823c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 1, 0, 1, 3, 2, 2, 3, 0, 3, 2, 1, 3, 2, 3, 4, 2, 2, 1, 2, 1, 3, 2,\n",
      "        0, 3, 4, 3, 3, 3, 1, 0], device='cuda:0')\n",
      "tensor([[ 0.1370,  0.0369, -0.3140, -0.0769,  0.2008],\n",
      "        [ 0.0680,  0.0955, -0.2299, -0.0780,  0.1754],\n",
      "        [-0.0983,  0.1429, -0.2284, -0.0066,  0.0506],\n",
      "        [ 0.0432,  0.1726, -0.2526, -0.0213,  0.1162],\n",
      "        [ 0.0540,  0.1167, -0.3187, -0.0821,  0.2123],\n",
      "        [ 0.1089,  0.1473, -0.2561, -0.1369,  0.1871],\n",
      "        [-0.0385,  0.1535, -0.2466, -0.0070,  0.0550],\n",
      "        [-0.0123,  0.1271, -0.2191, -0.0330,  0.1465],\n",
      "        [-0.0088,  0.1197, -0.2667,  0.0277,  0.0696],\n",
      "        [-0.0237,  0.1514, -0.2483, -0.0647,  0.1297],\n",
      "        [ 0.0525,  0.1411, -0.2403, -0.0590,  0.1217],\n",
      "        [-0.0090,  0.1142, -0.2299, -0.0382,  0.1277],\n",
      "        [-0.1220,  0.1796, -0.1817, -0.0036,  0.0230],\n",
      "        [ 0.1192,  0.0526, -0.3610,  0.0081,  0.1529],\n",
      "        [ 0.0293,  0.1438, -0.2093,  0.0523,  0.0152],\n",
      "        [ 0.0219,  0.1446, -0.1859, -0.0103,  0.1201],\n",
      "        [-0.0275,  0.1833, -0.2018, -0.0544,  0.1111],\n",
      "        [ 0.1529,  0.1427, -0.1074,  0.0358,  0.2487],\n",
      "        [ 0.0450,  0.0864, -0.2961, -0.0269,  0.0681],\n",
      "        [ 0.0178,  0.1686, -0.3076,  0.0506,  0.0602],\n",
      "        [ 0.1021,  0.0315, -0.2891, -0.0340,  0.2181],\n",
      "        [-0.0200,  0.1489, -0.2699, -0.0615,  0.0883],\n",
      "        [ 0.0840,  0.1558, -0.1940, -0.0642,  0.1364],\n",
      "        [ 0.0332,  0.1650, -0.2208, -0.0022,  0.1043],\n",
      "        [-0.0078,  0.0895, -0.2329, -0.0101,  0.1360],\n",
      "        [ 0.0411,  0.0833, -0.3122,  0.0494,  0.0770],\n",
      "        [ 0.0663,  0.1026, -0.3166, -0.0113,  0.1320],\n",
      "        [ 0.1114,  0.0271, -0.2057,  0.0260,  0.1045],\n",
      "        [ 0.1093,  0.1348, -0.3476,  0.0073,  0.1756],\n",
      "        [ 0.0140,  0.1256, -0.3057, -0.0070,  0.0987],\n",
      "        [ 0.0385,  0.2074, -0.1591, -0.1161,  0.1188],\n",
      "        [ 0.0186,  0.0765, -0.2586, -0.0245,  0.1402]], device='cuda:0')\n",
      "Epoch 1, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "step = 10\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    curBatch = 0\n",
    "    for batch in val_loader:\n",
    "        # print(batch)\n",
    "        # break\n",
    "        optimizer.zero_grad()\n",
    "        curBatch += 1\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        # print(labels)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        print(labels)\n",
    "        print(logits)\n",
    "        break\n",
    "        # loss = outputs.loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # break\n",
    "        if curBatch % step == 0:\n",
    "            print(f\"Step {curBatch * batch_size}, Loss: {total_loss / curBatch}\")\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f\"Epoch {1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1b343d1-3ba0-47ea-821a-9c054b873b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 out of 32\n"
     ]
    }
   ],
   "source": [
    "logitLabels = torch.argmax(logits, dim=1)\n",
    "matches = (logitLabels == labels).sum()\n",
    "print(f\"{matches} out of {len(logitLabels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcef56-1a5f-4164-84df-17f7482731e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "step_loss = 0\n",
    "for n in dataset[\"train\"]:\n",
    "    optimizer.zero_grad()\n",
    "    encoding = tokenizer(n[\"sentence\"], return_tensors='pt', padding=True, truncation=True)\n",
    "    logits = model(encoding['input_ids'], encoding['attention_mask'])\n",
    "    print(logits)\n",
    "    labels = [0, 0, 0]\n",
    "    labels[n[\"label\"]] = 1\n",
    "    loss = criterion(logits[0], torch.tensor(labels, dtype=torch.float32))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc8acf-c4a8-46ef-b197-6ffb94e9e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for n in dataset[\"train\"]:\n",
    "    print(n)\n",
    "    encoding = tokenizer(n[\"sentence\"], return_tensors='pt', padding=True, truncation=True)\n",
    "    logits = model(encoding['input_ids'], encoding['attention_mask'])\n",
    "    print(logits)\n",
    "    x += 1\n",
    "    if x > 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
