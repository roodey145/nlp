{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50d639a",
   "metadata": {},
   "source": [
    "# Settings Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e083d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "MODEL_NAME = \"V7\"\n",
    "\n",
    "EPOCHS = 15\n",
    "NUM_LABELS = 5\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cdac1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad90beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149bf27",
   "metadata": {},
   "source": [
    "# Connect GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff66860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2f842",
   "metadata": {},
   "source": [
    "# Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffb5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./models/\"   # local folder\n",
    "version_dir = os.path.join(BASE_PATH, MODEL_NAME)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    if not os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' does not exist.\")\n",
    "else:\n",
    "    if os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' already exists.\")\n",
    "    os.makedirs(version_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e1a92",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67b0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_log_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "combined_log_file = open(combined_log_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    combined_log_file.write(msg + \"\\n\")\n",
    "    combined_log_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddc4a",
   "metadata": {},
   "source": [
    "# Load and Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd5dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'very negative', 'positive', 'neutral', 'very positive', 'negative'}\n",
      "\n",
      "Train distribution:\n",
      "very positive: 1288\n",
      "negative: 2218\n",
      "neutral: 1624\n",
      "positive: 2322\n",
      "very negative: 1092\n",
      "\n",
      "Val distribution:\n",
      "neutral: 229\n",
      "negative: 289\n",
      "very negative: 139\n",
      "positive: 279\n",
      "very positive: 165\n",
      "\n",
      "Test distribution:\n",
      "negative: 633\n",
      "very negative: 279\n",
      "neutral: 389\n",
      "very positive: 399\n",
      "positive: 510\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/sst5\")\n",
    "\n",
    "print(\"Labels:\", set(dataset[\"train\"][\"label_text\"]))\n",
    "\n",
    "def print_dist(ds, name):\n",
    "    counts = Counter(ds['label_text'])\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for k,v in counts.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print_dist(dataset[\"train\"], \"Train\")\n",
    "print_dist(dataset[\"validation\"], \"Val\")\n",
    "print_dist(dataset[\"test\"], \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3945a7",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509369dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "datasetMap = dataset.map(tokenize, batched=True)\n",
    "datasetMap = datasetMap.rename_column(\"label\", \"labels\")\n",
    "datasetMap.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "train_loader = DataLoader(datasetMap[\"train\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(datasetMap[\"validation\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(datasetMap[\"test\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82614f35",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500bb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V1 - Base model \n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]  # CLS\n",
    "#         return self.classifier(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b594bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V2 — BERT + small MLP classifier head\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(self.bert.config.hidden_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fa3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === V3 — BERT + MLP head + partial freezing (best next step) ===\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Load base BERT\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # ----- PARTIAL FREEZING (V3 upgrade) -----\n",
    "#         # Freeze bottom 8 encoder layers\n",
    "#         for layer in self.bert.encoder.layer[:8]:\n",
    "#             for param in layer.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         # Keep the top 4 layers trainable\n",
    "#         # (no action needed; they default to requires_grad=True)\n",
    "\n",
    "#         # ----- Improved classifier head (inherits V2 but slightly more stable) -----\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.LayerNorm(self.bert.config.hidden_size),\n",
    "#             nn.Linear(self.bert.config.hidden_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d33fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V4 — V3 + GELU activations\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1317ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V5 — V4 + partial BERT freezing\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # Freeze all BERT layers except the last one\n",
    "#         for param in self.bert.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.bert.encoder.layer[-1].parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a341bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V6 — V5 + LayerNorm stabilization\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # Same partial freeze as V5\n",
    "#         for p in self.bert.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         for p in self.bert.encoder.layer[-1].parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(hidden),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af90f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V7 — V6 + CLS projection layer for long sequences\n",
    "class CustomBertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=NUM_LABELS):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Same partial freeze\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.bert.encoder.layer[-1].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        hidden = self.bert.config.hidden_size\n",
    "\n",
    "        self.project = nn.Linear(hidden, 384)  # reduces noise from long sequences\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(384),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        cls = self.project(cls)  # <-- NEW\n",
    "        return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a2ba5",
   "metadata": {},
   "source": [
    "# Load or Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d135f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBertClassifier(NUM_LABELS).to(DEVICE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ckpt = torch.load(os.path.join(version_dir, \"model.pt\"), map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "else:\n",
    "    model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bd5e8",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df97fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, crit, device, sample_print=500):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    next_print = sample_print\n",
    "\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = crit(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if total >= next_print:\n",
    "            log(f\"[Train] {total}/{len(loader.dataset)} \"\n",
    "                f\"| Loss={total_loss/step:.4f} | Acc={correct/total:.4f}\")\n",
    "            next_print += sample_print\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate(model, loader, crit, device, sample_print=500):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    next_print = sample_print\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(loader, 1):\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = crit(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            if total >= next_print:\n",
    "                log(f\"[Val] {total}/{len(loader.dataset)} \"\n",
    "                    f\"| Loss={total_loss/step:.4f} | Acc={correct/total:.4f}\")\n",
    "                next_print += sample_print\n",
    "\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 512/8544 | Loss=1.6004 | Acc=0.2500\n",
      "[Train] 1008/8544 | Loss=1.5887 | Acc=0.2579\n",
      "[Train] 1504/8544 | Loss=1.5785 | Acc=0.2773\n",
      "[Train] 2000/8544 | Loss=1.5738 | Acc=0.2860\n",
      "[Train] 2512/8544 | Loss=1.5600 | Acc=0.3073\n",
      "[Train] 3008/8544 | Loss=1.5514 | Acc=0.3185\n",
      "[Train] 3504/8544 | Loss=1.5399 | Acc=0.3251\n",
      "[Train] 4000/8544 | Loss=1.5232 | Acc=0.3342\n",
      "[Train] 4512/8544 | Loss=1.5062 | Acc=0.3429\n",
      "[Train] 5008/8544 | Loss=1.4897 | Acc=0.3508\n",
      "[Train] 5504/8544 | Loss=1.4738 | Acc=0.3568\n",
      "[Train] 6000/8544 | Loss=1.4569 | Acc=0.3650\n",
      "[Train] 6512/8544 | Loss=1.4409 | Acc=0.3744\n",
      "[Train] 7008/8544 | Loss=1.4286 | Acc=0.3799\n",
      "[Train] 7504/8544 | Loss=1.4170 | Acc=0.3835\n",
      "[Train] 8000/8544 | Loss=1.4064 | Acc=0.3884\n",
      "[Train] 8512/8544 | Loss=1.3944 | Acc=0.3940\n",
      "[Val] 512/1101 | Loss=1.1913 | Acc=0.4590\n",
      "[Val] 1008/1101 | Loss=1.1928 | Acc=0.4593\n",
      "Epoch 1: Train Acc=0.3942 | Val Acc=0.4550\n",
      "[Train] 512/8544 | Loss=1.2260 | Acc=0.4688\n",
      "[Train] 1008/8544 | Loss=1.2220 | Acc=0.4603\n",
      "[Train] 1504/8544 | Loss=1.2055 | Acc=0.4707\n",
      "[Train] 2000/8544 | Loss=1.2068 | Acc=0.4720\n",
      "[Train] 2512/8544 | Loss=1.2110 | Acc=0.4733\n",
      "[Train] 3008/8544 | Loss=1.2102 | Acc=0.4694\n",
      "[Train] 3504/8544 | Loss=1.2048 | Acc=0.4726\n",
      "[Train] 4000/8544 | Loss=1.2053 | Acc=0.4765\n",
      "[Train] 4512/8544 | Loss=1.2039 | Acc=0.4743\n",
      "[Train] 5008/8544 | Loss=1.2030 | Acc=0.4746\n",
      "[Train] 5504/8544 | Loss=1.2010 | Acc=0.4764\n",
      "[Train] 6000/8544 | Loss=1.1995 | Acc=0.4765\n",
      "[Train] 6512/8544 | Loss=1.1973 | Acc=0.4791\n",
      "[Train] 7008/8544 | Loss=1.1935 | Acc=0.4802\n",
      "[Train] 7504/8544 | Loss=1.1907 | Acc=0.4817\n",
      "[Train] 8000/8544 | Loss=1.1913 | Acc=0.4794\n",
      "[Train] 8512/8544 | Loss=1.1931 | Acc=0.4797\n",
      "[Val] 512/1101 | Loss=1.1379 | Acc=0.4785\n",
      "[Val] 1008/1101 | Loss=1.1394 | Acc=0.4831\n",
      "Epoch 2: Train Acc=0.4796 | Val Acc=0.4796\n",
      "[Train] 512/8544 | Loss=1.1811 | Acc=0.4883\n",
      "[Train] 1008/8544 | Loss=1.1355 | Acc=0.5169\n",
      "[Train] 1504/8544 | Loss=1.1499 | Acc=0.5027\n",
      "[Train] 2000/8544 | Loss=1.1506 | Acc=0.5050\n",
      "[Train] 2512/8544 | Loss=1.1446 | Acc=0.5048\n",
      "[Train] 3008/8544 | Loss=1.1463 | Acc=0.5057\n",
      "[Train] 3504/8544 | Loss=1.1544 | Acc=0.4994\n",
      "[Train] 4000/8544 | Loss=1.1541 | Acc=0.5028\n",
      "[Train] 4512/8544 | Loss=1.1525 | Acc=0.5007\n",
      "[Train] 5008/8544 | Loss=1.1525 | Acc=0.5012\n",
      "[Train] 5504/8544 | Loss=1.1488 | Acc=0.5035\n",
      "[Train] 6000/8544 | Loss=1.1477 | Acc=0.5048\n",
      "[Train] 6512/8544 | Loss=1.1501 | Acc=0.5032\n",
      "[Train] 7008/8544 | Loss=1.1478 | Acc=0.5041\n",
      "[Train] 7504/8544 | Loss=1.1493 | Acc=0.5036\n",
      "[Train] 8000/8544 | Loss=1.1487 | Acc=0.5029\n",
      "[Train] 8512/8544 | Loss=1.1461 | Acc=0.5036\n",
      "[Val] 512/1101 | Loss=1.1373 | Acc=0.4863\n",
      "[Val] 1008/1101 | Loss=1.1556 | Acc=0.4861\n",
      "Epoch 3: Train Acc=0.5039 | Val Acc=0.4768\n",
      "[Train] 512/8544 | Loss=1.1561 | Acc=0.4844\n",
      "[Train] 1008/8544 | Loss=1.1577 | Acc=0.4821\n",
      "[Train] 1504/8544 | Loss=1.1284 | Acc=0.5040\n",
      "[Train] 2000/8544 | Loss=1.1199 | Acc=0.5110\n",
      "[Train] 2512/8544 | Loss=1.1251 | Acc=0.5111\n",
      "[Train] 3008/8544 | Loss=1.1263 | Acc=0.5020\n",
      "[Train] 3504/8544 | Loss=1.1296 | Acc=0.4980\n",
      "[Train] 4000/8544 | Loss=1.1309 | Acc=0.5020\n",
      "[Train] 4512/8544 | Loss=1.1259 | Acc=0.5064\n",
      "[Train] 5008/8544 | Loss=1.1238 | Acc=0.5072\n",
      "[Train] 5504/8544 | Loss=1.1204 | Acc=0.5087\n",
      "[Train] 6000/8544 | Loss=1.1176 | Acc=0.5103\n",
      "[Train] 6512/8544 | Loss=1.1156 | Acc=0.5109\n",
      "[Train] 7008/8544 | Loss=1.1149 | Acc=0.5118\n",
      "[Train] 7504/8544 | Loss=1.1139 | Acc=0.5105\n",
      "[Train] 8000/8544 | Loss=1.1184 | Acc=0.5071\n",
      "[Train] 8512/8544 | Loss=1.1169 | Acc=0.5093\n",
      "[Val] 512/1101 | Loss=1.1113 | Acc=0.4941\n",
      "[Val] 1008/1101 | Loss=1.1251 | Acc=0.5069\n",
      "Epoch 4: Train Acc=0.5097 | Val Acc=0.4950\n",
      "[Train] 512/8544 | Loss=1.0881 | Acc=0.5234\n",
      "[Train] 1008/8544 | Loss=1.0935 | Acc=0.5258\n",
      "[Train] 1504/8544 | Loss=1.0880 | Acc=0.5366\n",
      "[Train] 2000/8544 | Loss=1.0892 | Acc=0.5335\n",
      "[Train] 2512/8544 | Loss=1.0916 | Acc=0.5295\n",
      "[Train] 3008/8544 | Loss=1.0895 | Acc=0.5279\n",
      "[Train] 3504/8544 | Loss=1.0890 | Acc=0.5291\n",
      "[Train] 4000/8544 | Loss=1.0848 | Acc=0.5325\n",
      "[Train] 4512/8544 | Loss=1.0811 | Acc=0.5357\n",
      "[Train] 5008/8544 | Loss=1.0819 | Acc=0.5315\n",
      "[Train] 5504/8544 | Loss=1.0863 | Acc=0.5285\n",
      "[Train] 6000/8544 | Loss=1.0837 | Acc=0.5290\n",
      "[Train] 6512/8544 | Loss=1.0874 | Acc=0.5295\n",
      "[Train] 7008/8544 | Loss=1.0856 | Acc=0.5294\n",
      "[Train] 7504/8544 | Loss=1.0886 | Acc=0.5275\n",
      "[Train] 8000/8544 | Loss=1.0914 | Acc=0.5265\n",
      "[Train] 8512/8544 | Loss=1.0895 | Acc=0.5274\n",
      "[Val] 512/1101 | Loss=1.1031 | Acc=0.5098\n",
      "[Val] 1008/1101 | Loss=1.1260 | Acc=0.5020\n",
      "Epoch 5: Train Acc=0.5276 | Val Acc=0.4914\n",
      "[Train] 512/8544 | Loss=1.0404 | Acc=0.5703\n",
      "[Train] 1008/8544 | Loss=1.0617 | Acc=0.5506\n",
      "[Train] 1504/8544 | Loss=1.0603 | Acc=0.5499\n",
      "[Train] 2000/8544 | Loss=1.0546 | Acc=0.5495\n",
      "[Train] 2512/8544 | Loss=1.0532 | Acc=0.5450\n",
      "[Train] 3008/8544 | Loss=1.0561 | Acc=0.5455\n",
      "[Train] 3504/8544 | Loss=1.0571 | Acc=0.5437\n",
      "[Train] 4000/8544 | Loss=1.0590 | Acc=0.5413\n",
      "[Train] 4512/8544 | Loss=1.0591 | Acc=0.5412\n",
      "[Train] 5008/8544 | Loss=1.0582 | Acc=0.5379\n",
      "[Train] 5504/8544 | Loss=1.0579 | Acc=0.5378\n",
      "[Train] 6000/8544 | Loss=1.0548 | Acc=0.5392\n",
      "[Train] 6512/8544 | Loss=1.0582 | Acc=0.5375\n",
      "[Train] 7008/8544 | Loss=1.0594 | Acc=0.5372\n",
      "[Train] 7504/8544 | Loss=1.0609 | Acc=0.5364\n",
      "[Train] 8000/8544 | Loss=1.0642 | Acc=0.5349\n",
      "[Train] 8512/8544 | Loss=1.0639 | Acc=0.5351\n",
      "[Val] 512/1101 | Loss=1.0998 | Acc=0.5039\n",
      "[Val] 1008/1101 | Loss=1.1160 | Acc=0.5079\n",
      "Epoch 6: Train Acc=0.5357 | Val Acc=0.4950\n",
      "[Train] 512/8544 | Loss=1.0379 | Acc=0.5527\n",
      "[Train] 1008/8544 | Loss=1.0357 | Acc=0.5536\n",
      "[Train] 1504/8544 | Loss=1.0293 | Acc=0.5638\n",
      "[Train] 2000/8544 | Loss=1.0362 | Acc=0.5605\n",
      "[Train] 2512/8544 | Loss=1.0307 | Acc=0.5621\n",
      "[Train] 3008/8544 | Loss=1.0214 | Acc=0.5668\n",
      "[Train] 3504/8544 | Loss=1.0315 | Acc=0.5574\n",
      "[Train] 4000/8544 | Loss=1.0244 | Acc=0.5613\n",
      "[Train] 4512/8544 | Loss=1.0291 | Acc=0.5587\n",
      "[Train] 5008/8544 | Loss=1.0286 | Acc=0.5575\n",
      "[Train] 5504/8544 | Loss=1.0285 | Acc=0.5600\n",
      "[Train] 6000/8544 | Loss=1.0296 | Acc=0.5608\n",
      "[Train] 6512/8544 | Loss=1.0286 | Acc=0.5584\n",
      "[Train] 7008/8544 | Loss=1.0267 | Acc=0.5586\n",
      "[Train] 7504/8544 | Loss=1.0264 | Acc=0.5594\n",
      "[Train] 8000/8544 | Loss=1.0270 | Acc=0.5581\n",
      "[Train] 8512/8544 | Loss=1.0258 | Acc=0.5591\n",
      "[Val] 512/1101 | Loss=1.1002 | Acc=0.5137\n",
      "[Val] 1008/1101 | Loss=1.1213 | Acc=0.5099\n",
      "Epoch 7: Train Acc=0.5593 | Val Acc=0.4977\n",
      "[Train] 512/8544 | Loss=0.9702 | Acc=0.5781\n",
      "[Train] 1008/8544 | Loss=0.9902 | Acc=0.5774\n",
      "[Train] 1504/8544 | Loss=0.9987 | Acc=0.5751\n",
      "[Train] 2000/8544 | Loss=0.9926 | Acc=0.5800\n",
      "[Train] 2512/8544 | Loss=1.0059 | Acc=0.5717\n",
      "[Train] 3008/8544 | Loss=0.9994 | Acc=0.5751\n",
      "[Train] 3504/8544 | Loss=0.9954 | Acc=0.5751\n",
      "[Train] 4000/8544 | Loss=1.0004 | Acc=0.5700\n",
      "[Train] 4512/8544 | Loss=0.9941 | Acc=0.5709\n",
      "[Train] 5008/8544 | Loss=0.9896 | Acc=0.5717\n",
      "[Train] 5504/8544 | Loss=0.9903 | Acc=0.5681\n",
      "[Train] 6000/8544 | Loss=0.9908 | Acc=0.5697\n",
      "[Train] 6512/8544 | Loss=0.9932 | Acc=0.5674\n",
      "[Train] 7008/8544 | Loss=0.9919 | Acc=0.5676\n",
      "[Train] 7504/8544 | Loss=0.9915 | Acc=0.5684\n",
      "[Train] 8000/8544 | Loss=0.9920 | Acc=0.5690\n",
      "[Train] 8512/8544 | Loss=0.9925 | Acc=0.5705\n",
      "[Val] 512/1101 | Loss=1.1322 | Acc=0.5039\n",
      "[Val] 1008/1101 | Loss=1.1647 | Acc=0.4990\n",
      "Epoch 8: Train Acc=0.5701 | Val Acc=0.4896\n",
      "[Train] 512/8544 | Loss=0.9932 | Acc=0.5625\n",
      "[Train] 1008/8544 | Loss=0.9740 | Acc=0.5764\n",
      "[Train] 1504/8544 | Loss=0.9741 | Acc=0.5805\n",
      "[Train] 2000/8544 | Loss=0.9651 | Acc=0.5825\n",
      "[Train] 2512/8544 | Loss=0.9649 | Acc=0.5812\n",
      "[Train] 3008/8544 | Loss=0.9737 | Acc=0.5795\n",
      "[Train] 3504/8544 | Loss=0.9709 | Acc=0.5825\n",
      "[Train] 4000/8544 | Loss=0.9736 | Acc=0.5807\n",
      "[Train] 4512/8544 | Loss=0.9745 | Acc=0.5791\n",
      "[Train] 5008/8544 | Loss=0.9716 | Acc=0.5809\n",
      "[Train] 5504/8544 | Loss=0.9719 | Acc=0.5830\n",
      "[Train] 6000/8544 | Loss=0.9747 | Acc=0.5822\n",
      "[Train] 6512/8544 | Loss=0.9736 | Acc=0.5809\n",
      "[Train] 7008/8544 | Loss=0.9712 | Acc=0.5813\n",
      "[Train] 7504/8544 | Loss=0.9672 | Acc=0.5829\n",
      "[Train] 8000/8544 | Loss=0.9680 | Acc=0.5829\n",
      "[Train] 8512/8544 | Loss=0.9698 | Acc=0.5822\n",
      "[Val] 512/1101 | Loss=1.1266 | Acc=0.4941\n",
      "[Val] 1008/1101 | Loss=1.1511 | Acc=0.5000\n",
      "Epoch 9: Train Acc=0.5819 | Val Acc=0.4896\n",
      "[Train] 512/8544 | Loss=0.9496 | Acc=0.5820\n",
      "[Train] 1008/8544 | Loss=0.9418 | Acc=0.5903\n",
      "[Train] 1504/8544 | Loss=0.9419 | Acc=0.5838\n",
      "[Train] 2000/8544 | Loss=0.9450 | Acc=0.5850\n",
      "[Train] 2512/8544 | Loss=0.9440 | Acc=0.5951\n",
      "[Train] 3008/8544 | Loss=0.9434 | Acc=0.5914\n",
      "[Train] 3504/8544 | Loss=0.9449 | Acc=0.5913\n",
      "[Train] 4000/8544 | Loss=0.9363 | Acc=0.5960\n",
      "[Train] 4512/8544 | Loss=0.9361 | Acc=0.5962\n",
      "[Train] 5008/8544 | Loss=0.9323 | Acc=0.5992\n",
      "[Train] 5504/8544 | Loss=0.9308 | Acc=0.5988\n",
      "[Train] 6000/8544 | Loss=0.9312 | Acc=0.5992\n",
      "[Train] 6512/8544 | Loss=0.9321 | Acc=0.5972\n",
      "[Train] 7008/8544 | Loss=0.9352 | Acc=0.5970\n",
      "[Train] 7504/8544 | Loss=0.9349 | Acc=0.5973\n",
      "[Train] 8000/8544 | Loss=0.9364 | Acc=0.5966\n",
      "[Train] 8512/8544 | Loss=0.9346 | Acc=0.5976\n",
      "[Val] 512/1101 | Loss=1.1255 | Acc=0.5020\n",
      "[Val] 1008/1101 | Loss=1.1457 | Acc=0.5000\n",
      "Epoch 10: Train Acc=0.5977 | Val Acc=0.4905\n",
      "[Train] 512/8544 | Loss=0.8759 | Acc=0.6289\n",
      "[Train] 1008/8544 | Loss=0.8988 | Acc=0.6270\n",
      "[Train] 1504/8544 | Loss=0.8985 | Acc=0.6177\n",
      "[Train] 2000/8544 | Loss=0.9080 | Acc=0.6155\n",
      "[Train] 2512/8544 | Loss=0.9052 | Acc=0.6186\n",
      "[Train] 3008/8544 | Loss=0.8983 | Acc=0.6260\n",
      "[Train] 3504/8544 | Loss=0.8924 | Acc=0.6290\n",
      "[Train] 4000/8544 | Loss=0.8936 | Acc=0.6288\n",
      "[Train] 4512/8544 | Loss=0.8916 | Acc=0.6277\n",
      "[Train] 5008/8544 | Loss=0.8903 | Acc=0.6282\n",
      "[Train] 5504/8544 | Loss=0.8906 | Acc=0.6281\n",
      "[Train] 6000/8544 | Loss=0.8918 | Acc=0.6280\n",
      "[Train] 6512/8544 | Loss=0.8942 | Acc=0.6270\n",
      "[Train] 7008/8544 | Loss=0.8956 | Acc=0.6233\n",
      "[Train] 7504/8544 | Loss=0.8946 | Acc=0.6245\n",
      "[Train] 8000/8544 | Loss=0.8926 | Acc=0.6244\n",
      "[Train] 8512/8544 | Loss=0.8957 | Acc=0.6230\n",
      "[Val] 512/1101 | Loss=1.1689 | Acc=0.4980\n",
      "[Val] 1008/1101 | Loss=1.1786 | Acc=0.4940\n",
      "Epoch 11: Train Acc=0.6228 | Val Acc=0.4850\n",
      "[Train] 512/8544 | Loss=0.8871 | Acc=0.6348\n",
      "[Train] 1008/8544 | Loss=0.8522 | Acc=0.6548\n",
      "[Train] 1504/8544 | Loss=0.8524 | Acc=0.6516\n",
      "[Train] 2000/8544 | Loss=0.8514 | Acc=0.6405\n",
      "[Train] 2512/8544 | Loss=0.8522 | Acc=0.6405\n",
      "[Train] 3008/8544 | Loss=0.8555 | Acc=0.6356\n",
      "[Train] 3504/8544 | Loss=0.8479 | Acc=0.6398\n",
      "[Train] 4000/8544 | Loss=0.8516 | Acc=0.6425\n",
      "[Train] 4512/8544 | Loss=0.8667 | Acc=0.6343\n",
      "[Train] 5008/8544 | Loss=0.8626 | Acc=0.6340\n",
      "[Train] 5504/8544 | Loss=0.8589 | Acc=0.6374\n",
      "[Train] 6000/8544 | Loss=0.8570 | Acc=0.6383\n",
      "[Train] 6512/8544 | Loss=0.8570 | Acc=0.6388\n",
      "[Train] 7008/8544 | Loss=0.8583 | Acc=0.6380\n",
      "[Train] 7504/8544 | Loss=0.8578 | Acc=0.6365\n",
      "[Train] 8000/8544 | Loss=0.8596 | Acc=0.6358\n",
      "[Train] 8512/8544 | Loss=0.8596 | Acc=0.6353\n",
      "[Val] 512/1101 | Loss=1.2075 | Acc=0.5059\n",
      "[Val] 1008/1101 | Loss=1.2273 | Acc=0.5040\n",
      "Epoch 12: Train Acc=0.6355 | Val Acc=0.4941\n",
      "[Train] 512/8544 | Loss=0.7576 | Acc=0.6855\n",
      "[Train] 1008/8544 | Loss=0.7671 | Acc=0.6726\n",
      "[Train] 1504/8544 | Loss=0.7866 | Acc=0.6642\n",
      "[Train] 2000/8544 | Loss=0.7904 | Acc=0.6595\n",
      "[Train] 2512/8544 | Loss=0.7967 | Acc=0.6644\n",
      "[Train] 3008/8544 | Loss=0.7985 | Acc=0.6659\n",
      "[Train] 3504/8544 | Loss=0.7978 | Acc=0.6661\n",
      "[Train] 4000/8544 | Loss=0.8122 | Acc=0.6605\n",
      "[Train] 4512/8544 | Loss=0.8196 | Acc=0.6556\n",
      "[Train] 5008/8544 | Loss=0.8180 | Acc=0.6556\n",
      "[Train] 5504/8544 | Loss=0.8143 | Acc=0.6588\n",
      "[Train] 6000/8544 | Loss=0.8108 | Acc=0.6617\n",
      "[Train] 6512/8544 | Loss=0.8109 | Acc=0.6622\n",
      "[Train] 7008/8544 | Loss=0.8102 | Acc=0.6628\n",
      "[Train] 7504/8544 | Loss=0.8114 | Acc=0.6628\n",
      "[Train] 8000/8544 | Loss=0.8167 | Acc=0.6594\n",
      "[Train] 8512/8544 | Loss=0.8183 | Acc=0.6581\n",
      "[Val] 512/1101 | Loss=1.2357 | Acc=0.4961\n",
      "[Val] 1008/1101 | Loss=1.2672 | Acc=0.4881\n",
      "Epoch 13: Train Acc=0.6586 | Val Acc=0.4859\n",
      "[Train] 512/8544 | Loss=0.7491 | Acc=0.7031\n",
      "[Train] 1008/8544 | Loss=0.7429 | Acc=0.7093\n",
      "[Train] 1504/8544 | Loss=0.7515 | Acc=0.7041\n",
      "[Train] 2000/8544 | Loss=0.7456 | Acc=0.7055\n",
      "[Train] 2512/8544 | Loss=0.7491 | Acc=0.7078\n",
      "[Train] 3008/8544 | Loss=0.7481 | Acc=0.7084\n",
      "[Train] 3504/8544 | Loss=0.7507 | Acc=0.7032\n",
      "[Train] 4000/8544 | Loss=0.7566 | Acc=0.6977\n",
      "[Train] 4512/8544 | Loss=0.7562 | Acc=0.6966\n",
      "[Train] 5008/8544 | Loss=0.7607 | Acc=0.6945\n",
      "[Train] 5504/8544 | Loss=0.7658 | Acc=0.6939\n",
      "[Train] 6000/8544 | Loss=0.7622 | Acc=0.6942\n",
      "[Train] 6512/8544 | Loss=0.7622 | Acc=0.6929\n",
      "[Train] 7008/8544 | Loss=0.7647 | Acc=0.6909\n",
      "[Train] 7504/8544 | Loss=0.7694 | Acc=0.6883\n",
      "[Train] 8000/8544 | Loss=0.7690 | Acc=0.6857\n",
      "[Train] 8512/8544 | Loss=0.7697 | Acc=0.6859\n",
      "[Val] 512/1101 | Loss=1.2728 | Acc=0.5020\n",
      "[Val] 1008/1101 | Loss=1.2910 | Acc=0.4911\n",
      "Epoch 14: Train Acc=0.6863 | Val Acc=0.4850\n",
      "[Train] 512/8544 | Loss=0.7229 | Acc=0.6895\n",
      "[Train] 1008/8544 | Loss=0.7467 | Acc=0.6984\n",
      "[Train] 1504/8544 | Loss=0.7346 | Acc=0.7114\n",
      "[Train] 2000/8544 | Loss=0.7399 | Acc=0.7015\n",
      "[Train] 2512/8544 | Loss=0.7373 | Acc=0.7018\n",
      "[Train] 3008/8544 | Loss=0.7297 | Acc=0.7048\n",
      "[Train] 3504/8544 | Loss=0.7386 | Acc=0.7006\n",
      "[Train] 4000/8544 | Loss=0.7317 | Acc=0.7017\n",
      "[Train] 4512/8544 | Loss=0.7347 | Acc=0.6999\n",
      "[Train] 5008/8544 | Loss=0.7360 | Acc=0.7017\n",
      "[Train] 5504/8544 | Loss=0.7371 | Acc=0.6982\n",
      "[Train] 6000/8544 | Loss=0.7379 | Acc=0.6975\n",
      "[Train] 6512/8544 | Loss=0.7371 | Acc=0.6981\n",
      "[Train] 7008/8544 | Loss=0.7383 | Acc=0.6971\n",
      "[Train] 7504/8544 | Loss=0.7378 | Acc=0.6976\n",
      "[Train] 8000/8544 | Loss=0.7349 | Acc=0.6993\n",
      "[Train] 8512/8544 | Loss=0.7353 | Acc=0.6982\n",
      "[Val] 512/1101 | Loss=1.3173 | Acc=0.5039\n",
      "[Val] 1008/1101 | Loss=1.3337 | Acc=0.5010\n",
      "Epoch 15: Train Acc=0.6979 | Val Acc=0.4950\n"
     ]
    }
   ],
   "source": [
    "# Metrics to track\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, val_acc     = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    # <-- Add these lines\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee93ce",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5009049773755656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4341    0.4014    0.4171       279\n",
      "           1     0.5241    0.6003    0.5596       633\n",
      "           2     0.3544    0.2879    0.3177       389\n",
      "           3     0.5028    0.5216    0.5120       510\n",
      "           4     0.6204    0.5940    0.6069       399\n",
      "\n",
      "    accuracy                         0.5009      2210\n",
      "   macro avg     0.4872    0.4810    0.4827      2210\n",
      "weighted avg     0.4954    0.5009    0.4966      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = crit(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds.extend(logits.argmax(1).cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    return total_loss/len(loader), preds, labels_list\n",
    "\n",
    "test_loss, preds, labels_list = test(model, test_loader, criterion, DEVICE)\n",
    "test_acc = accuracy_score(labels_list, preds)\n",
    "report = classification_report(labels_list, preds, digits=4)\n",
    "cm = confusion_matrix(labels_list, preds)\n",
    "\n",
    "# Build FINAL RESULTS block\n",
    "final_results_text = (\n",
    "    \"========== FINAL RESULTS ==========\\n\"\n",
    "    f\"Model Version: {MODEL_NAME}\\n\\n\"\n",
    "    f\"Final Train Accuracy: {train_acc:.4f}\\n\"\n",
    "    f\"Final Validation Accuracy: {val_acc:.4f}\\n\\n\"\n",
    "    f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"Test Accuracy: {test_acc:.4f}\\n\\n\"\n",
    "    \"Classification Report:\\n\"\n",
    "    f\"{report}\\n\"\n",
    "    \"====================================\\n\\n\"\n",
    ")\n",
    "\n",
    "# Path to your output file\n",
    "out_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "\n",
    "# Read the old content\n",
    "try:\n",
    "    with open(out_path, \"r\") as f:\n",
    "        old_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    old_content = \"\"\n",
    "\n",
    "# Write FINAL RESULTS at top, followed by original content\n",
    "with open(out_path, \"w\") as f:\n",
    "    f.write(final_results_text + old_content)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bc204",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65bb1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOT & SAVE TRAINING CURVES ===\n",
    "\n",
    "# epochs list\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# --- create a combined figure ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- LOSS PLOT ---\n",
    "ax[0].plot(epochs, train_losses, label=\"Train Loss\")\n",
    "ax[0].plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "ax[0].set_title(\"Training and Validation Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- ACCURACY PLOT ---\n",
    "ax[1].plot(epochs, train_accs, label=\"Train Accuracy\")\n",
    "ax[1].plot(epochs, val_accs, label=\"Validation Accuracy\")\n",
    "ax[1].set_title(\"Training and Validation Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# === SAVE to version folder ===\n",
    "curve_path = os.path.join(version_dir, \"training_curves.png\")\n",
    "fig.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ac79d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(version_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c94d9",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict()},\n",
    "           os.path.join(version_dir, \"model.pt\"))\n",
    "\n",
    "with open(os.path.join(version_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"LR\": LR,\n",
    "        \"MAX_LEN\": MAX_LEN,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE\n",
    "    }, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
