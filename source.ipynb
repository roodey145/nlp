{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50d639a",
   "metadata": {},
   "source": [
    "# Settings Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e083d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "MODEL_NAME = \"V8\"\n",
    "\n",
    "EPOCHS = 15\n",
    "NUM_LABELS = 5\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cdac1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad90beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149bf27",
   "metadata": {},
   "source": [
    "# Connect GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aff66860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2f842",
   "metadata": {},
   "source": [
    "# Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cffb5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./models/\"   # local folder\n",
    "version_dir = os.path.join(BASE_PATH, MODEL_NAME)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    if not os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' does not exist.\")\n",
    "else:\n",
    "    if os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' already exists.\")\n",
    "    os.makedirs(version_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e1a92",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f67b0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_log_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "combined_log_file = open(combined_log_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    combined_log_file.write(msg + \"\\n\")\n",
    "    combined_log_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddc4a",
   "metadata": {},
   "source": [
    "# Load and Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd5dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels:\n",
      "0: very negative\n",
      "1: negative\n",
      "2: neutral\n",
      "3: positive\n",
      "4: very positive\n",
      "\n",
      "Train distribution:\n",
      "very positive: 1288\n",
      "negative: 2218\n",
      "neutral: 1624\n",
      "positive: 2322\n",
      "very negative: 1092\n",
      "\n",
      "Val distribution:\n",
      "neutral: 229\n",
      "negative: 289\n",
      "very negative: 139\n",
      "positive: 279\n",
      "very positive: 165\n",
      "\n",
      "Test distribution:\n",
      "negative: 633\n",
      "very negative: 279\n",
      "neutral: 389\n",
      "very positive: 399\n",
      "positive: 510\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/sst5\")\n",
    "\n",
    "def printLabels():\n",
    "    print(\"\\nLabels:\")\n",
    "    id_to_label = dict(sorted({i: t for i, t in zip(dataset[\"train\"][\"label\"], dataset[\"train\"][\"label_text\"])}.items()))\n",
    "\n",
    "    for k, v in id_to_label.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "def print_dist(ds, name):\n",
    "    counts = Counter(ds['label_text'])\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for k,v in counts.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "printLabels()\n",
    "print_dist(dataset[\"train\"], \"Train\")\n",
    "print_dist(dataset[\"validation\"], \"Val\")\n",
    "print_dist(dataset[\"test\"], \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3945a7",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509369dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "datasetMap = dataset.map(tokenize, batched=True)\n",
    "datasetMap = datasetMap.rename_column(\"label\", \"labels\")\n",
    "datasetMap.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "train_loader = DataLoader(datasetMap[\"train\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(datasetMap[\"validation\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(datasetMap[\"test\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82614f35",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "500bb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V1 - Base model \n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]  # CLS\n",
    "#         return self.classifier(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b594bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # V2 — BERT + small MLP classifier head\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(self.bert.config.hidden_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model variation is generated by Chat-GPT.\n",
    "\n",
    "# # === V3 — BERT + MLP head + partial freezing (best next step) ===\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Load base BERT\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # ----- PARTIAL FREEZING (V3 upgrade) -----\n",
    "#         # Freeze bottom 8 encoder layers\n",
    "#         for layer in self.bert.encoder.layer[:8]:\n",
    "#             for param in layer.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#         # Keep the top 4 layers trainable\n",
    "#         # (no action needed; they default to requires_grad=True)\n",
    "\n",
    "#         # ----- Improved classifier head (inherits V2 but slightly more stable) -----\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.LayerNorm(self.bert.config.hidden_size),\n",
    "#             nn.Linear(self.bert.config.hidden_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = outputs.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d33fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model variation is generated by Chat-GPT.\n",
    "\n",
    "# # V4 — V3 + GELU activations\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model variation is generated by Chat-GPT.\n",
    "\n",
    "# # V5 — V4 + partial BERT freezing\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # Freeze all BERT layers except the last one\n",
    "#         for param in self.bert.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.bert.encoder.layer[-1].parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a341bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model variation is generated by Chat-GPT.\n",
    "\n",
    "# V6 — V5 + LayerNorm stabilization\n",
    "class CustomBertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=NUM_LABELS):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Same partial freeze as V5\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.bert.encoder.layer[-1].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        hidden = self.bert.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af90f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model variation is generated by Chat-GPT.\n",
    "\n",
    "# # V7 — V6 + CLS projection layer for long sequences\n",
    "# class CustomBertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels=NUM_LABELS):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#         # Same partial freeze\n",
    "#         for p in self.bert.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         for p in self.bert.encoder.layer[-1].parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#         hidden = self.bert.config.hidden_size\n",
    "\n",
    "#         self.project = nn.Linear(hidden, 384)  # reduces noise from long sequences\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(384),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(384, 256),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(128, num_labels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0, :]\n",
    "#         cls = self.project(cls)  # <-- NEW\n",
    "#         return self.classifier(cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a2ba5",
   "metadata": {},
   "source": [
    "# Load or Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d135f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBertClassifier(NUM_LABELS).to(DEVICE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ckpt = torch.load(os.path.join(version_dir, \"model.pt\"), map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "else:\n",
    "    model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bd5e8",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6df97fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, crit, device, sample_print=500):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    next_print = sample_print\n",
    "\n",
    "    for step, batch in enumerate(loader, 1):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = crit(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if total >= next_print:\n",
    "            log(f\"[Train] {total}/{len(loader.dataset)} \"\n",
    "                f\"| Loss={total_loss/step:.4f} | Acc={correct/total:.4f}\")\n",
    "            next_print += sample_print\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "# def train_one_epoch(model, loader, opt, device, sample_print=500):\n",
    "#     model.train()\n",
    "#     total_loss, correct, total = 0, 0, 0\n",
    "#     next_print = sample_print\n",
    "\n",
    "#     for step, batch in enumerate(loader, 1):\n",
    "#         opt.zero_grad()\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=batch[\"input_ids\"].to(device),\n",
    "#             attention_mask=batch[\"attention_mask\"].to(device),\n",
    "#             #labels=batch[\"labels\"].to(device)\n",
    "#         )\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         preds = logits.argmax(1)\n",
    "#         correct += (preds == batch[\"labels\"].to(device)).sum().item()\n",
    "#         total += batch[\"labels\"].size(0)\n",
    "\n",
    "#     return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate(model, loader, crit, device, sample_print=500):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    next_print = sample_print\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(loader, 1):\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = crit(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            if total >= next_print:\n",
    "                log(f\"[Val] {total}/{len(loader.dataset)} \"\n",
    "                    f\"| Loss={total_loss/step:.4f} | Acc={correct/total:.4f}\")\n",
    "                next_print += sample_print\n",
    "\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e3cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 512/8544 | Loss=1.5970 | Acc=0.2441\n",
      "[Train] 1008/8544 | Loss=1.5882 | Acc=0.2460\n",
      "[Train] 1504/8544 | Loss=1.5832 | Acc=0.2626\n",
      "[Train] 2000/8544 | Loss=1.5725 | Acc=0.2800\n",
      "[Train] 2512/8544 | Loss=1.5669 | Acc=0.2910\n",
      "[Train] 3008/8544 | Loss=1.5576 | Acc=0.3049\n",
      "[Train] 3504/8544 | Loss=1.5510 | Acc=0.3116\n",
      "[Train] 4000/8544 | Loss=1.5391 | Acc=0.3230\n",
      "[Train] 4512/8544 | Loss=1.5248 | Acc=0.3338\n",
      "[Train] 5008/8544 | Loss=1.5060 | Acc=0.3435\n",
      "[Train] 5504/8544 | Loss=1.4900 | Acc=0.3530\n",
      "[Train] 6000/8544 | Loss=1.4730 | Acc=0.3602\n",
      "[Train] 6512/8544 | Loss=1.4602 | Acc=0.3644\n",
      "[Train] 7008/8544 | Loss=1.4420 | Acc=0.3743\n",
      "[Train] 7504/8544 | Loss=1.4270 | Acc=0.3803\n",
      "[Train] 8000/8544 | Loss=1.4142 | Acc=0.3830\n",
      "[Train] 8512/8544 | Loss=1.4017 | Acc=0.3878\n",
      "[Val] 512/1101 | Loss=1.1891 | Acc=0.4727\n",
      "[Val] 1008/1101 | Loss=1.2022 | Acc=0.4603\n",
      "Epoch 1: Train Acc=0.3887 | Val Acc=0.4578\n",
      "[Train] 512/8544 | Loss=1.1878 | Acc=0.4902\n",
      "[Train] 1008/8544 | Loss=1.1804 | Acc=0.4881\n",
      "[Train] 1504/8544 | Loss=1.1784 | Acc=0.4920\n",
      "[Train] 2000/8544 | Loss=1.1927 | Acc=0.4815\n",
      "[Train] 2512/8544 | Loss=1.1951 | Acc=0.4809\n",
      "[Train] 3008/8544 | Loss=1.1958 | Acc=0.4791\n",
      "[Train] 3504/8544 | Loss=1.1932 | Acc=0.4806\n",
      "[Train] 4000/8544 | Loss=1.1930 | Acc=0.4773\n",
      "[Train] 4512/8544 | Loss=1.1901 | Acc=0.4798\n",
      "[Train] 5008/8544 | Loss=1.1858 | Acc=0.4840\n",
      "[Train] 5504/8544 | Loss=1.1906 | Acc=0.4786\n",
      "[Train] 6000/8544 | Loss=1.1884 | Acc=0.4813\n",
      "[Train] 6512/8544 | Loss=1.1890 | Acc=0.4797\n",
      "[Train] 7008/8544 | Loss=1.1896 | Acc=0.4805\n",
      "[Train] 7504/8544 | Loss=1.1852 | Acc=0.4805\n",
      "[Train] 8000/8544 | Loss=1.1827 | Acc=0.4806\n",
      "[Train] 8512/8544 | Loss=1.1851 | Acc=0.4799\n",
      "[Val] 512/1101 | Loss=1.1361 | Acc=0.4746\n",
      "[Val] 1008/1101 | Loss=1.1488 | Acc=0.4861\n",
      "Epoch 2: Train Acc=0.4793 | Val Acc=0.4805\n",
      "[Train] 512/8544 | Loss=1.1124 | Acc=0.4922\n",
      "[Train] 1008/8544 | Loss=1.1033 | Acc=0.5089\n",
      "[Train] 1504/8544 | Loss=1.1111 | Acc=0.5047\n",
      "[Train] 2000/8544 | Loss=1.1239 | Acc=0.5050\n",
      "[Train] 2512/8544 | Loss=1.1283 | Acc=0.5044\n",
      "[Train] 3008/8544 | Loss=1.1324 | Acc=0.5007\n",
      "[Train] 3504/8544 | Loss=1.1309 | Acc=0.5057\n",
      "[Train] 4000/8544 | Loss=1.1343 | Acc=0.5070\n",
      "[Train] 4512/8544 | Loss=1.1348 | Acc=0.5044\n",
      "[Train] 5008/8544 | Loss=1.1378 | Acc=0.5034\n",
      "[Train] 5504/8544 | Loss=1.1393 | Acc=0.5049\n",
      "[Train] 6000/8544 | Loss=1.1394 | Acc=0.5025\n",
      "[Train] 6512/8544 | Loss=1.1355 | Acc=0.5029\n",
      "[Train] 7008/8544 | Loss=1.1366 | Acc=0.5020\n",
      "[Train] 7504/8544 | Loss=1.1346 | Acc=0.5040\n",
      "[Train] 8000/8544 | Loss=1.1346 | Acc=0.5042\n",
      "[Train] 8512/8544 | Loss=1.1352 | Acc=0.5016\n",
      "[Val] 512/1101 | Loss=1.1238 | Acc=0.4863\n",
      "[Val] 1008/1101 | Loss=1.1292 | Acc=0.5040\n",
      "Epoch 3: Train Acc=0.5016 | Val Acc=0.4932\n",
      "[Train] 512/8544 | Loss=1.1146 | Acc=0.5098\n",
      "[Train] 1008/8544 | Loss=1.1199 | Acc=0.5159\n",
      "[Train] 1504/8544 | Loss=1.1130 | Acc=0.5273\n",
      "[Train] 2000/8544 | Loss=1.1092 | Acc=0.5270\n",
      "[Train] 2512/8544 | Loss=1.1077 | Acc=0.5247\n",
      "[Train] 3008/8544 | Loss=1.1111 | Acc=0.5199\n",
      "[Train] 3504/8544 | Loss=1.1088 | Acc=0.5188\n",
      "[Train] 4000/8544 | Loss=1.1111 | Acc=0.5188\n",
      "[Train] 4512/8544 | Loss=1.1159 | Acc=0.5146\n",
      "[Train] 5008/8544 | Loss=1.1181 | Acc=0.5112\n",
      "[Train] 5504/8544 | Loss=1.1204 | Acc=0.5093\n",
      "[Train] 6000/8544 | Loss=1.1195 | Acc=0.5093\n",
      "[Train] 6512/8544 | Loss=1.1151 | Acc=0.5094\n",
      "[Train] 7008/8544 | Loss=1.1153 | Acc=0.5077\n",
      "[Train] 7504/8544 | Loss=1.1157 | Acc=0.5085\n",
      "[Train] 8000/8544 | Loss=1.1139 | Acc=0.5096\n",
      "[Train] 8512/8544 | Loss=1.1083 | Acc=0.5135\n",
      "[Val] 512/1101 | Loss=1.1179 | Acc=0.4980\n",
      "[Val] 1008/1101 | Loss=1.1306 | Acc=0.5159\n",
      "Epoch 4: Train Acc=0.5136 | Val Acc=0.5068\n",
      "[Train] 512/8544 | Loss=1.1097 | Acc=0.4980\n",
      "[Train] 1008/8544 | Loss=1.1028 | Acc=0.5258\n",
      "[Train] 1504/8544 | Loss=1.0880 | Acc=0.5352\n",
      "[Train] 2000/8544 | Loss=1.1056 | Acc=0.5305\n",
      "[Train] 2512/8544 | Loss=1.1098 | Acc=0.5187\n",
      "[Train] 3008/8544 | Loss=1.1098 | Acc=0.5156\n",
      "[Train] 3504/8544 | Loss=1.1136 | Acc=0.5166\n",
      "[Train] 4000/8544 | Loss=1.1080 | Acc=0.5152\n",
      "[Train] 4512/8544 | Loss=1.0972 | Acc=0.5204\n",
      "[Train] 5008/8544 | Loss=1.0931 | Acc=0.5230\n",
      "[Train] 5504/8544 | Loss=1.0910 | Acc=0.5273\n",
      "[Train] 6000/8544 | Loss=1.0946 | Acc=0.5247\n",
      "[Train] 6512/8544 | Loss=1.0950 | Acc=0.5244\n",
      "[Train] 7008/8544 | Loss=1.0947 | Acc=0.5273\n",
      "[Train] 7504/8544 | Loss=1.0909 | Acc=0.5297\n",
      "[Train] 8000/8544 | Loss=1.0881 | Acc=0.5311\n",
      "[Train] 8512/8544 | Loss=1.0871 | Acc=0.5314\n",
      "[Val] 512/1101 | Loss=1.1245 | Acc=0.4941\n",
      "[Val] 1008/1101 | Loss=1.1359 | Acc=0.5129\n",
      "Epoch 5: Train Acc=0.5317 | Val Acc=0.5005\n",
      "[Train] 512/8544 | Loss=1.1375 | Acc=0.4961\n",
      "[Train] 1008/8544 | Loss=1.0945 | Acc=0.5258\n",
      "[Train] 1504/8544 | Loss=1.0924 | Acc=0.5259\n",
      "[Train] 2000/8544 | Loss=1.0847 | Acc=0.5285\n",
      "[Train] 2512/8544 | Loss=1.0636 | Acc=0.5378\n",
      "[Train] 3008/8544 | Loss=1.0634 | Acc=0.5419\n",
      "[Train] 3504/8544 | Loss=1.0669 | Acc=0.5420\n",
      "[Train] 4000/8544 | Loss=1.0706 | Acc=0.5385\n",
      "[Train] 4512/8544 | Loss=1.0723 | Acc=0.5377\n",
      "[Train] 5008/8544 | Loss=1.0752 | Acc=0.5357\n",
      "[Train] 5504/8544 | Loss=1.0712 | Acc=0.5380\n",
      "[Train] 6000/8544 | Loss=1.0664 | Acc=0.5408\n",
      "[Train] 6512/8544 | Loss=1.0641 | Acc=0.5427\n",
      "[Train] 7008/8544 | Loss=1.0659 | Acc=0.5412\n",
      "[Train] 7504/8544 | Loss=1.0664 | Acc=0.5392\n",
      "[Train] 8000/8544 | Loss=1.0668 | Acc=0.5396\n",
      "[Train] 8512/8544 | Loss=1.0641 | Acc=0.5397\n",
      "[Val] 512/1101 | Loss=1.1194 | Acc=0.5000\n",
      "[Val] 1008/1101 | Loss=1.1249 | Acc=0.5139\n",
      "Epoch 6: Train Acc=0.5397 | Val Acc=0.5005\n",
      "[Train] 512/8544 | Loss=1.0113 | Acc=0.5645\n",
      "[Train] 1008/8544 | Loss=1.0132 | Acc=0.5556\n",
      "[Train] 1504/8544 | Loss=1.0162 | Acc=0.5559\n",
      "[Train] 2000/8544 | Loss=1.0347 | Acc=0.5535\n",
      "[Train] 2512/8544 | Loss=1.0358 | Acc=0.5537\n",
      "[Train] 3008/8544 | Loss=1.0366 | Acc=0.5525\n",
      "[Train] 3504/8544 | Loss=1.0377 | Acc=0.5537\n",
      "[Train] 4000/8544 | Loss=1.0382 | Acc=0.5540\n",
      "[Train] 4512/8544 | Loss=1.0320 | Acc=0.5567\n",
      "[Train] 5008/8544 | Loss=1.0353 | Acc=0.5581\n",
      "[Train] 5504/8544 | Loss=1.0324 | Acc=0.5590\n",
      "[Train] 6000/8544 | Loss=1.0360 | Acc=0.5540\n",
      "[Train] 6512/8544 | Loss=1.0388 | Acc=0.5522\n",
      "[Train] 7008/8544 | Loss=1.0431 | Acc=0.5495\n",
      "[Train] 7504/8544 | Loss=1.0438 | Acc=0.5458\n",
      "[Train] 8000/8544 | Loss=1.0439 | Acc=0.5459\n",
      "[Train] 8512/8544 | Loss=1.0435 | Acc=0.5458\n",
      "[Val] 512/1101 | Loss=1.1024 | Acc=0.5273\n",
      "[Val] 1008/1101 | Loss=1.1127 | Acc=0.5298\n",
      "Epoch 7: Train Acc=0.5466 | Val Acc=0.5186\n",
      "[Train] 512/8544 | Loss=1.0014 | Acc=0.5664\n",
      "[Train] 1008/8544 | Loss=1.0182 | Acc=0.5546\n",
      "[Train] 1504/8544 | Loss=0.9960 | Acc=0.5632\n",
      "[Train] 2000/8544 | Loss=0.9968 | Acc=0.5605\n",
      "[Train] 2512/8544 | Loss=0.9953 | Acc=0.5609\n",
      "[Train] 3008/8544 | Loss=1.0101 | Acc=0.5535\n",
      "[Train] 3504/8544 | Loss=1.0127 | Acc=0.5528\n",
      "[Train] 4000/8544 | Loss=1.0151 | Acc=0.5513\n",
      "[Train] 4512/8544 | Loss=1.0146 | Acc=0.5521\n",
      "[Train] 5008/8544 | Loss=1.0095 | Acc=0.5561\n",
      "[Train] 5504/8544 | Loss=1.0089 | Acc=0.5569\n",
      "[Train] 6000/8544 | Loss=1.0126 | Acc=0.5557\n",
      "[Train] 6512/8544 | Loss=1.0151 | Acc=0.5561\n",
      "[Train] 7008/8544 | Loss=1.0148 | Acc=0.5575\n",
      "[Train] 7504/8544 | Loss=1.0150 | Acc=0.5576\n",
      "[Train] 8000/8544 | Loss=1.0143 | Acc=0.5600\n",
      "[Train] 8512/8544 | Loss=1.0155 | Acc=0.5590\n",
      "[Val] 512/1101 | Loss=1.1073 | Acc=0.5020\n",
      "[Val] 1008/1101 | Loss=1.1316 | Acc=0.5119\n",
      "Epoch 8: Train Acc=0.5583 | Val Acc=0.5014\n",
      "[Train] 512/8544 | Loss=0.9582 | Acc=0.5859\n",
      "[Train] 1008/8544 | Loss=0.9675 | Acc=0.5794\n",
      "[Train] 1504/8544 | Loss=0.9846 | Acc=0.5798\n",
      "[Train] 2000/8544 | Loss=0.9896 | Acc=0.5770\n",
      "[Train] 2512/8544 | Loss=0.9875 | Acc=0.5760\n",
      "[Train] 3008/8544 | Loss=0.9871 | Acc=0.5728\n",
      "[Train] 3504/8544 | Loss=0.9857 | Acc=0.5728\n",
      "[Train] 4000/8544 | Loss=0.9916 | Acc=0.5715\n",
      "[Train] 4512/8544 | Loss=0.9915 | Acc=0.5714\n",
      "[Train] 5008/8544 | Loss=0.9925 | Acc=0.5689\n",
      "[Train] 5504/8544 | Loss=0.9905 | Acc=0.5721\n",
      "[Train] 6000/8544 | Loss=0.9926 | Acc=0.5695\n",
      "[Train] 6512/8544 | Loss=0.9907 | Acc=0.5725\n",
      "[Train] 7008/8544 | Loss=0.9936 | Acc=0.5726\n",
      "[Train] 7504/8544 | Loss=0.9952 | Acc=0.5733\n",
      "[Train] 8000/8544 | Loss=0.9959 | Acc=0.5741\n",
      "[Train] 8512/8544 | Loss=0.9934 | Acc=0.5744\n",
      "[Val] 512/1101 | Loss=1.1213 | Acc=0.5117\n",
      "[Val] 1008/1101 | Loss=1.1363 | Acc=0.5198\n",
      "Epoch 9: Train Acc=0.5742 | Val Acc=0.5086\n",
      "[Train] 512/8544 | Loss=0.9621 | Acc=0.5781\n",
      "[Train] 1008/8544 | Loss=0.9944 | Acc=0.5675\n",
      "[Train] 1504/8544 | Loss=0.9793 | Acc=0.5691\n",
      "[Train] 2000/8544 | Loss=0.9612 | Acc=0.5820\n",
      "[Train] 2512/8544 | Loss=0.9635 | Acc=0.5860\n",
      "[Train] 3008/8544 | Loss=0.9637 | Acc=0.5864\n",
      "[Train] 3504/8544 | Loss=0.9577 | Acc=0.5933\n",
      "[Train] 4000/8544 | Loss=0.9485 | Acc=0.5992\n",
      "[Train] 4512/8544 | Loss=0.9504 | Acc=0.5982\n",
      "[Train] 5008/8544 | Loss=0.9481 | Acc=0.6004\n",
      "[Train] 5504/8544 | Loss=0.9488 | Acc=0.5992\n",
      "[Train] 6000/8544 | Loss=0.9483 | Acc=0.5983\n",
      "[Train] 6512/8544 | Loss=0.9542 | Acc=0.5952\n",
      "[Train] 7008/8544 | Loss=0.9588 | Acc=0.5940\n",
      "[Train] 7504/8544 | Loss=0.9628 | Acc=0.5926\n",
      "[Train] 8000/8544 | Loss=0.9646 | Acc=0.5921\n",
      "[Train] 8512/8544 | Loss=0.9656 | Acc=0.5915\n",
      "[Val] 512/1101 | Loss=1.1529 | Acc=0.5078\n",
      "[Val] 1008/1101 | Loss=1.1677 | Acc=0.5169\n",
      "Epoch 10: Train Acc=0.5915 | Val Acc=0.5077\n",
      "[Train] 512/8544 | Loss=0.9747 | Acc=0.5742\n",
      "[Train] 1008/8544 | Loss=0.9653 | Acc=0.5833\n",
      "[Train] 1504/8544 | Loss=0.9773 | Acc=0.5805\n",
      "[Train] 2000/8544 | Loss=0.9763 | Acc=0.5885\n",
      "[Train] 2512/8544 | Loss=0.9659 | Acc=0.5912\n",
      "[Train] 3008/8544 | Loss=0.9644 | Acc=0.5941\n",
      "[Train] 3504/8544 | Loss=0.9573 | Acc=0.5973\n",
      "[Train] 4000/8544 | Loss=0.9501 | Acc=0.6005\n",
      "[Train] 4512/8544 | Loss=0.9462 | Acc=0.6028\n",
      "[Train] 5008/8544 | Loss=0.9498 | Acc=0.5988\n",
      "[Train] 5504/8544 | Loss=0.9509 | Acc=0.5970\n",
      "[Train] 6000/8544 | Loss=0.9472 | Acc=0.5998\n",
      "[Train] 6512/8544 | Loss=0.9467 | Acc=0.6006\n",
      "[Train] 7008/8544 | Loss=0.9445 | Acc=0.6003\n",
      "[Train] 7504/8544 | Loss=0.9484 | Acc=0.5995\n",
      "[Train] 8000/8544 | Loss=0.9436 | Acc=0.6030\n",
      "[Train] 8512/8544 | Loss=0.9432 | Acc=0.6033\n",
      "[Val] 512/1101 | Loss=1.1518 | Acc=0.5195\n",
      "[Val] 1008/1101 | Loss=1.1744 | Acc=0.5149\n",
      "Epoch 11: Train Acc=0.6036 | Val Acc=0.5077\n",
      "[Train] 512/8544 | Loss=0.8461 | Acc=0.6211\n",
      "[Train] 1008/8544 | Loss=0.8726 | Acc=0.6250\n",
      "[Train] 1504/8544 | Loss=0.8883 | Acc=0.6237\n",
      "[Train] 2000/8544 | Loss=0.8981 | Acc=0.6155\n",
      "[Train] 2512/8544 | Loss=0.8959 | Acc=0.6230\n",
      "[Train] 3008/8544 | Loss=0.8954 | Acc=0.6203\n",
      "[Train] 3504/8544 | Loss=0.8963 | Acc=0.6190\n",
      "[Train] 4000/8544 | Loss=0.8919 | Acc=0.6210\n",
      "[Train] 4512/8544 | Loss=0.8958 | Acc=0.6179\n",
      "[Train] 5008/8544 | Loss=0.8986 | Acc=0.6180\n",
      "[Train] 5504/8544 | Loss=0.8966 | Acc=0.6192\n",
      "[Train] 6000/8544 | Loss=0.8990 | Acc=0.6212\n",
      "[Train] 6512/8544 | Loss=0.9064 | Acc=0.6155\n",
      "[Train] 7008/8544 | Loss=0.9065 | Acc=0.6134\n",
      "[Train] 7504/8544 | Loss=0.9061 | Acc=0.6141\n",
      "[Train] 8000/8544 | Loss=0.9086 | Acc=0.6129\n",
      "[Train] 8512/8544 | Loss=0.9130 | Acc=0.6098\n",
      "[Val] 512/1101 | Loss=1.1488 | Acc=0.5020\n",
      "[Val] 1008/1101 | Loss=1.1668 | Acc=0.5149\n",
      "Epoch 12: Train Acc=0.6104 | Val Acc=0.5050\n",
      "[Train] 512/8544 | Loss=0.8514 | Acc=0.6582\n",
      "[Train] 1008/8544 | Loss=0.8610 | Acc=0.6379\n",
      "[Train] 1504/8544 | Loss=0.8710 | Acc=0.6443\n",
      "[Train] 2000/8544 | Loss=0.8726 | Acc=0.6290\n",
      "[Train] 2512/8544 | Loss=0.8738 | Acc=0.6258\n",
      "[Train] 3008/8544 | Loss=0.8738 | Acc=0.6260\n",
      "[Train] 3504/8544 | Loss=0.8678 | Acc=0.6313\n",
      "[Train] 4000/8544 | Loss=0.8658 | Acc=0.6318\n",
      "[Train] 4512/8544 | Loss=0.8675 | Acc=0.6292\n",
      "[Train] 5008/8544 | Loss=0.8685 | Acc=0.6274\n",
      "[Train] 5504/8544 | Loss=0.8681 | Acc=0.6299\n",
      "[Train] 6000/8544 | Loss=0.8684 | Acc=0.6307\n",
      "[Train] 6512/8544 | Loss=0.8711 | Acc=0.6311\n",
      "[Train] 7008/8544 | Loss=0.8704 | Acc=0.6307\n",
      "[Train] 7504/8544 | Loss=0.8698 | Acc=0.6299\n",
      "[Train] 8000/8544 | Loss=0.8719 | Acc=0.6271\n",
      "[Train] 8512/8544 | Loss=0.8764 | Acc=0.6236\n",
      "[Val] 512/1101 | Loss=1.1783 | Acc=0.4883\n",
      "[Val] 1008/1101 | Loss=1.2014 | Acc=0.4970\n",
      "Epoch 13: Train Acc=0.6234 | Val Acc=0.4877\n",
      "[Train] 512/8544 | Loss=0.8074 | Acc=0.6484\n",
      "[Train] 1008/8544 | Loss=0.8322 | Acc=0.6409\n",
      "[Train] 1504/8544 | Loss=0.8398 | Acc=0.6350\n",
      "[Train] 2000/8544 | Loss=0.8391 | Acc=0.6350\n",
      "[Train] 2512/8544 | Loss=0.8465 | Acc=0.6354\n",
      "[Train] 3008/8544 | Loss=0.8399 | Acc=0.6370\n",
      "[Train] 3504/8544 | Loss=0.8444 | Acc=0.6361\n",
      "[Train] 4000/8544 | Loss=0.8416 | Acc=0.6375\n",
      "[Train] 4512/8544 | Loss=0.8485 | Acc=0.6328\n",
      "[Train] 5008/8544 | Loss=0.8499 | Acc=0.6318\n",
      "[Train] 5504/8544 | Loss=0.8521 | Acc=0.6310\n",
      "[Train] 6000/8544 | Loss=0.8509 | Acc=0.6335\n",
      "[Train] 6512/8544 | Loss=0.8537 | Acc=0.6333\n",
      "[Train] 7008/8544 | Loss=0.8511 | Acc=0.6350\n",
      "[Train] 7504/8544 | Loss=0.8500 | Acc=0.6345\n",
      "[Train] 8000/8544 | Loss=0.8513 | Acc=0.6329\n",
      "[Train] 8512/8544 | Loss=0.8505 | Acc=0.6328\n",
      "[Val] 512/1101 | Loss=1.2009 | Acc=0.5020\n",
      "[Val] 1008/1101 | Loss=1.2316 | Acc=0.4990\n",
      "Epoch 14: Train Acc=0.6331 | Val Acc=0.4914\n",
      "[Train] 512/8544 | Loss=0.8162 | Acc=0.6562\n",
      "[Train] 1008/8544 | Loss=0.8133 | Acc=0.6528\n",
      "[Train] 1504/8544 | Loss=0.8268 | Acc=0.6509\n",
      "[Train] 2000/8544 | Loss=0.8320 | Acc=0.6475\n",
      "[Train] 2512/8544 | Loss=0.8286 | Acc=0.6509\n",
      "[Train] 3008/8544 | Loss=0.8210 | Acc=0.6592\n",
      "[Train] 3504/8544 | Loss=0.8228 | Acc=0.6578\n",
      "[Train] 4000/8544 | Loss=0.8191 | Acc=0.6610\n",
      "[Train] 4512/8544 | Loss=0.8167 | Acc=0.6598\n",
      "[Train] 5008/8544 | Loss=0.8162 | Acc=0.6583\n",
      "[Train] 5504/8544 | Loss=0.8185 | Acc=0.6566\n",
      "[Train] 6000/8544 | Loss=0.8207 | Acc=0.6543\n",
      "[Train] 6512/8544 | Loss=0.8184 | Acc=0.6568\n",
      "[Train] 7008/8544 | Loss=0.8157 | Acc=0.6568\n",
      "[Train] 7504/8544 | Loss=0.8150 | Acc=0.6586\n",
      "[Train] 8000/8544 | Loss=0.8194 | Acc=0.6565\n",
      "[Train] 8512/8544 | Loss=0.8176 | Acc=0.6572\n",
      "[Val] 512/1101 | Loss=1.2228 | Acc=0.4961\n",
      "[Val] 1008/1101 | Loss=1.2476 | Acc=0.5050\n",
      "Epoch 15: Train Acc=0.6572 | Val Acc=0.4950\n"
     ]
    }
   ],
   "source": [
    "# Metrics to track\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, val_acc     = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    # <-- Add these lines\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee93ce",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0745e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5117647058823529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4518    0.3692    0.4063       279\n",
      "           1     0.5341    0.6193    0.5735       633\n",
      "           2     0.3750    0.2853    0.3241       389\n",
      "           3     0.4983    0.5922    0.5412       510\n",
      "           4     0.6445    0.5589    0.5987       399\n",
      "\n",
      "    accuracy                         0.5118      2210\n",
      "   macro avg     0.5007    0.4850    0.4888      2210\n",
      "weighted avg     0.5074    0.5118    0.5056      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = crit(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds.extend(logits.argmax(1).cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    return total_loss/len(loader), preds, labels_list\n",
    "\n",
    "test_loss, preds, labels_list = test(model, test_loader, criterion, DEVICE)\n",
    "test_acc = accuracy_score(labels_list, preds)\n",
    "report = classification_report(labels_list, preds, digits=4)\n",
    "cm = confusion_matrix(labels_list, preds)\n",
    "\n",
    "# Build FINAL RESULTS block\n",
    "final_results_text = (\n",
    "    \"========== FINAL RESULTS ==========\\n\"\n",
    "    f\"Model Version: {MODEL_NAME}\\n\\n\"\n",
    "    f\"Final Train Accuracy: {train_acc:.4f}\\n\"\n",
    "    f\"Final Validation Accuracy: {val_acc:.4f}\\n\\n\"\n",
    "    f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"Test Accuracy: {test_acc:.4f}\\n\\n\"\n",
    "    \"Classification Report:\\n\"\n",
    "    f\"{report}\\n\"\n",
    "    \"====================================\\n\\n\"\n",
    ")\n",
    "\n",
    "# Path to your output file\n",
    "out_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "\n",
    "# Read the old content\n",
    "try:\n",
    "    with open(out_path, \"r\") as f:\n",
    "        old_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    old_content = \"\"\n",
    "\n",
    "# Write FINAL RESULTS at top, followed by original content\n",
    "with open(out_path, \"w\") as f:\n",
    "    f.write(final_results_text + old_content)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bc204",
   "metadata": {},
   "source": [
    "# Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65bb1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOT & SAVE TRAINING CURVES ===\n",
    "\n",
    "# epochs list\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# --- create a combined figure ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- LOSS PLOT ---\n",
    "ax[0].plot(epochs, train_losses, label=\"Train Loss\")\n",
    "ax[0].plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "ax[0].set_title(\"Training and Validation Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- ACCURACY PLOT ---\n",
    "ax[1].plot(epochs, train_accs, label=\"Train Accuracy\")\n",
    "ax[1].plot(epochs, val_accs, label=\"Validation Accuracy\")\n",
    "ax[1].set_title(\"Training and Validation Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# === SAVE to version folder ===\n",
    "curve_path = os.path.join(version_dir, \"training_curves.png\")\n",
    "fig.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac79d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(version_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c94d9",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc90ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict()},\n",
    "           os.path.join(version_dir, \"model.pt\"))\n",
    "\n",
    "with open(os.path.join(version_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"LR\": LR,\n",
    "        \"MAX_LEN\": MAX_LEN,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE\n",
    "    }, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
