{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111154a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# BERT + BiLSTM for SST-5\n",
    "# ============================\n",
    "\n",
    "!pip install transformers nlpaug datasets torch tqdm --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load SST-5 data\n",
    "# Expecting: label<TAB>sentence\n",
    "# -----------------------------\n",
    "def load_sst5(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, names=[\"label\",\"text\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = load_sst5(\"train.tsv\")\n",
    "dev_df   = load_sst5(\"dev.tsv\")\n",
    "test_df  = load_sst5(\"test.tsv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional NLPAUG augmentation\n",
    "# -----------------------------\n",
    "AUG = True   # set to False to disable augmentation\n",
    "if AUG:\n",
    "    aug = naw.SynonymAug(aug_src=\"wordnet\", aug_p=0.1)\n",
    "\n",
    "def maybe_augment(t):\n",
    "    if AUG:\n",
    "        return aug.augment(t)\n",
    "    return t\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset wrapper\n",
    "# -----------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class SST5Dataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.iloc[idx].text\n",
    "        label = self.df.iloc[idx].label\n",
    "\n",
    "        if self.augment:\n",
    "            sentence = maybe_augment(sentence)\n",
    "\n",
    "        enc = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }\n",
    "\n",
    "train_ds = SST5Dataset(train_df, augment=True)\n",
    "dev_ds   = SST5Dataset(dev_df, augment=False)\n",
    "test_ds  = SST5Dataset(test_df, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_ds, batch_size=32)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# -----------------------------\n",
    "# Model: BERT → BiLSTM → linear\n",
    "# -----------------------------\n",
    "class BERT_BiLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden=256, layers=1, num_classes=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if layers > 1 else 0\n",
    "        )\n",
    "        self.out = nn.Linear(lstm_hidden*2, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        return self.out(hidden)\n",
    "\n",
    "model = BERT_BiLSTM().to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Training setup\n",
    "# -----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# -----------------------------\n",
    "# Eval helper\n",
    "# -----------------------------\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            mask = batch[\"mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        mask = batch[\"mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_acc = evaluate(dev_loader)\n",
    "    print(f\"Dev accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final test accuracy\n",
    "# -----------------------------\n",
    "test_acc = evaluate(test_loader)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
