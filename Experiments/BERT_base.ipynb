{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb90eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "MODEL_NAME = \"V0_ZERO_SHOT\"\n",
    "\n",
    "EPOCHS = 15\n",
    "NUM_LABELS = 5\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f703fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2571e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8edf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./models/\"\n",
    "version_dir = os.path.join(BASE_PATH, MODEL_NAME)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    if not os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' does not exist.\")\n",
    "else:\n",
    "    if os.path.exists(version_dir):\n",
    "        raise RuntimeError(f\"Model '{MODEL_NAME}' already exists.\")\n",
    "    os.makedirs(version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad84ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_log_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "combined_log_file = open(combined_log_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    combined_log_file.write(msg + \"\\n\")\n",
    "    combined_log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90efb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"SetFit/sst5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bdfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'neutral', 'very negative', 'negative', 'positive', 'very positive'}\n",
      "\n",
      "Train distribution:\n",
      "very positive: 1288\n",
      "negative: 2218\n",
      "neutral: 1624\n",
      "positive: 2322\n",
      "very negative: 1092\n",
      "\n",
      "Val distribution:\n",
      "neutral: 229\n",
      "negative: 289\n",
      "very negative: 139\n",
      "positive: 279\n",
      "very positive: 165\n",
      "\n",
      "Test distribution:\n",
      "negative: 633\n",
      "very negative: 279\n",
      "neutral: 389\n",
      "very positive: 399\n",
      "positive: 510\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", set(dataset[\"train\"][\"label_text\"]))\n",
    "\n",
    "def print_dist(ds, name):\n",
    "    counts = Counter(ds['label_text'])\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for k,v in counts.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print_dist(dataset[\"train\"], \"Train\")\n",
    "print_dist(dataset[\"validation\"], \"Val\")\n",
    "print_dist(dataset[\"test\"], \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aece6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "datasetMap = dataset.map(tokenize, batched=True)\n",
    "datasetMap = datasetMap.rename_column(\"label\", \"labels\")\n",
    "datasetMap.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "train_loader = DataLoader(datasetMap[\"train\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(datasetMap[\"validation\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(datasetMap[\"test\"], batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdd1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=NUM_LABELS):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Freeze BERT completely\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            out = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "223993e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBertClassifier(NUM_LABELS).to(DEVICE)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ckpt = torch.load(os.path.join(version_dir, \"model.pt\"), map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "else:\n",
    "    model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd5d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, crit, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(ids, mask)\n",
    "        loss = crit(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            loss = crit(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de6a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=0.3978 | Val Acc=0.4296\n",
      "Epoch 2: Train Acc=0.4566 | Val Acc=0.4269\n",
      "Epoch 3: Train Acc=0.4696 | Val Acc=0.4360\n",
      "Epoch 4: Train Acc=0.4760 | Val Acc=0.4559\n",
      "Epoch 5: Train Acc=0.4775 | Val Acc=0.4687\n",
      "Epoch 6: Train Acc=0.4875 | Val Acc=0.4578\n",
      "Epoch 7: Train Acc=0.4774 | Val Acc=0.4450\n",
      "Epoch 8: Train Acc=0.4886 | Val Acc=0.4687\n",
      "Epoch 9: Train Acc=0.4918 | Val Acc=0.4578\n",
      "Epoch 10: Train Acc=0.4886 | Val Acc=0.4614\n",
      "Epoch 11: Train Acc=0.4943 | Val Acc=0.4668\n",
      "Epoch 12: Train Acc=0.4905 | Val Acc=0.4777\n",
      "Epoch 13: Train Acc=0.4930 | Val Acc=0.4587\n",
      "Epoch 14: Train Acc=0.4938 | Val Acc=0.4659\n",
      "Epoch 15: Train Acc=0.4901 | Val Acc=0.4460\n"
     ]
    }
   ],
   "source": [
    "# Track Metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, val_acc     = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    # <-- Add these lines\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.47692307692307695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4186    0.4516    0.4345       279\n",
      "           1     0.5047    0.5972    0.5470       633\n",
      "           2     0.3732    0.1362    0.1996       389\n",
      "           3     0.4373    0.5196    0.4749       510\n",
      "           4     0.5631    0.5815    0.5721       399\n",
      "\n",
      "    accuracy                         0.4769      2210\n",
      "   macro avg     0.4594    0.4572    0.4456      2210\n",
      "weighted avg     0.4657    0.4769    0.4596      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(model, loader, crit, device):\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = crit(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds.extend(logits.argmax(1).cpu().tolist())\n",
    "            labels_list.extend(labels.cpu().tolist())\n",
    "\n",
    "    return total_loss/len(loader), preds, labels_list\n",
    "\n",
    "test_loss, preds, labels_list = test(model, test_loader, criterion, DEVICE)\n",
    "test_acc = accuracy_score(labels_list, preds)\n",
    "report = classification_report(labels_list, preds, digits=4)\n",
    "cm = confusion_matrix(labels_list, preds)\n",
    "\n",
    "# Build FINAL RESULTS block\n",
    "final_results_text = (\n",
    "    \"========== FINAL RESULTS ==========\\n\"\n",
    "    f\"Model Version: {MODEL_NAME}\\n\\n\"\n",
    "    f\"Final Train Accuracy: {train_acc:.4f}\\n\"\n",
    "    f\"Final Validation Accuracy: {val_acc:.4f}\\n\\n\"\n",
    "    f\"Test Loss: {test_loss:.4f}\\n\"\n",
    "    f\"Test Accuracy: {test_acc:.4f}\\n\\n\"\n",
    "    \"Classification Report:\\n\"\n",
    "    f\"{report}\\n\"\n",
    "    \"====================================\\n\\n\"\n",
    ")\n",
    "\n",
    "# Path to your output file\n",
    "out_path = os.path.join(version_dir, \"run_output.txt\")\n",
    "\n",
    "# Read the old content\n",
    "try:\n",
    "    with open(out_path, \"r\") as f:\n",
    "        old_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    old_content = \"\"\n",
    "\n",
    "# Write FINAL RESULTS at top, followed by original content\n",
    "with open(out_path, \"w\") as f:\n",
    "    f.write(final_results_text + old_content)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "280e5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOT & SAVE TRAINING CURVES ===\n",
    "\n",
    "# epochs list\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "# --- create a combined figure ---\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- LOSS PLOT ---\n",
    "ax[0].plot(epochs, train_losses, label=\"Train Loss\")\n",
    "ax[0].plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "ax[0].set_title(\"Training and Validation Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "# --- ACCURACY PLOT ---\n",
    "ax[1].plot(epochs, train_accs, label=\"Train Accuracy\")\n",
    "ax[1].plot(epochs, val_accs, label=\"Validation Accuracy\")\n",
    "ax[1].set_title(\"Training and Validation Accuracy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# === SAVE to version folder ===\n",
    "curve_path = os.path.join(version_dir, \"training_curves.png\")\n",
    "fig.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e7b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(version_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d77e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict()},\n",
    "os.path.join(version_dir, \"model.pt\"))\n",
    "\n",
    "with open(os.path.join(version_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE\n",
    "    }, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
